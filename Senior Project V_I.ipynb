{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "from tensorflow import device\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 210, 210, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 210, 210, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 210, 210, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 105, 105, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 105, 105, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 105, 105, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 52, 52, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 52, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 52, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 26, 26, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 26, 26, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(210,210, 3))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\feras\\\\Bovine data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 21\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "Found 84 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 6, 6, 512))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    \n",
    "    if sample_count == 540:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1/255,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.2)\n",
    "    else:\n",
    "        train_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = train_datagen.flow_from_directory(directory,\n",
    "        target_size=(210,210),\n",
    "        batch_size = batch_size, \n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "train_features, train_labels = extract_features(train_dir, 540)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 60)\n",
    "test_features, test_labels = extract_features(test_dir, 84)\n",
    "##----------------------------------------------------------\n",
    "train_features = np.reshape(train_features, (540, 6* 6* 512))\n",
    "validation_features = np.reshape(validation_features, (60, 6* 6* 512))\n",
    "test_features = np.reshape(test_features, (84, 6* 6* 512))\n",
    "inputs = np.concatenate((train_features, test_features), axis=0)\n",
    "targets = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 16ms/step - loss: 1.1333 - acc: 0.3307 - val_loss: 1.0952 - val_acc: 0.3000\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1204 - acc: 0.3012 - val_loss: 1.0968 - val_acc: 0.3667\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1131 - acc: 0.3588 - val_loss: 1.0970 - val_acc: 0.3667\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1035 - acc: 0.3874 - val_loss: 1.0981 - val_acc: 0.3667\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1191 - acc: 0.3474 - val_loss: 1.1001 - val_acc: 0.3000\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1097 - acc: 0.3153 - val_loss: 1.0965 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0986 - acc: 0.3213 - val_loss: 1.0960 - val_acc: 0.3667\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0955 - acc: 0.3287 - val_loss: 1.0971 - val_acc: 0.3667\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1019 - acc: 0.3403 - val_loss: 1.0973 - val_acc: 0.3333\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1006 - acc: 0.3200 - val_loss: 1.0966 - val_acc: 0.3833\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0987 - acc: 0.3440 - val_loss: 1.0963 - val_acc: 0.3000\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1009 - acc: 0.3601 - val_loss: 1.0958 - val_acc: 0.3500\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0950 - acc: 0.3428 - val_loss: 1.0960 - val_acc: 0.3333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1000 - acc: 0.3287 - val_loss: 1.0978 - val_acc: 0.3333\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0990 - acc: 0.3423 - val_loss: 1.0964 - val_acc: 0.3167\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0967 - acc: 0.3893 - val_loss: 1.0954 - val_acc: 0.3500\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0896 - acc: 0.3852 - val_loss: 1.0951 - val_acc: 0.4167\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0965 - acc: 0.3533 - val_loss: 1.0962 - val_acc: 0.3500\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0970 - acc: 0.3601 - val_loss: 1.0973 - val_acc: 0.3333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0885 - acc: 0.3836 - val_loss: 1.0968 - val_acc: 0.3167\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0995 - acc: 0.3138 - val_loss: 1.0940 - val_acc: 0.3667\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0960 - acc: 0.3679 - val_loss: 1.0933 - val_acc: 0.4167\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0931 - acc: 0.3857 - val_loss: 1.0956 - val_acc: 0.3500\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0956 - acc: 0.3585 - val_loss: 1.0944 - val_acc: 0.4167\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0904 - acc: 0.3728 - val_loss: 1.0974 - val_acc: 0.3833\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0879 - acc: 0.4079 - val_loss: 1.0941 - val_acc: 0.4000\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0844 - acc: 0.3769 - val_loss: 1.0927 - val_acc: 0.4333\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0934 - acc: 0.3870 - val_loss: 1.0916 - val_acc: 0.4167\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0939 - acc: 0.3091 - val_loss: 1.0910 - val_acc: 0.4167\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0669 - acc: 0.4402 - val_loss: 1.0923 - val_acc: 0.3833\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 16ms/step - loss: 1.1599 - acc: 0.3172 - val_loss: 1.1076 - val_acc: 0.3333\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1339 - acc: 0.3192 - val_loss: 1.1055 - val_acc: 0.3333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1127 - acc: 0.3225 - val_loss: 1.1001 - val_acc: 0.2667\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0955 - acc: 0.3460 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0869 - acc: 0.4157 - val_loss: 1.1025 - val_acc: 0.3667\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1083 - acc: 0.3035 - val_loss: 1.0979 - val_acc: 0.3000\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1007 - acc: 0.3558 - val_loss: 1.0973 - val_acc: 0.3500\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0882 - acc: 0.4315 - val_loss: 1.0999 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0949 - acc: 0.3750 - val_loss: 1.0996 - val_acc: 0.2833\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1044 - acc: 0.3168 - val_loss: 1.1004 - val_acc: 0.3167\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0959 - acc: 0.3762 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0997 - acc: 0.3286 - val_loss: 1.0978 - val_acc: 0.3500\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0971 - acc: 0.3395 - val_loss: 1.0990 - val_acc: 0.3667\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0968 - acc: 0.3406 - val_loss: 1.0994 - val_acc: 0.3500\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1009 - acc: 0.3317 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0994 - acc: 0.3139 - val_loss: 1.0983 - val_acc: 0.3000\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0936 - acc: 0.3828 - val_loss: 1.0993 - val_acc: 0.3500\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0999 - acc: 0.3174 - val_loss: 1.0979 - val_acc: 0.3333\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0945 - acc: 0.3797 - val_loss: 1.0967 - val_acc: 0.3667\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0996 - acc: 0.3407 - val_loss: 1.0976 - val_acc: 0.3833\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0896 - acc: 0.3662 - val_loss: 1.1008 - val_acc: 0.3333\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1072 - acc: 0.3106 - val_loss: 1.0980 - val_acc: 0.3333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0993 - acc: 0.3408 - val_loss: 1.0984 - val_acc: 0.2667\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0898 - acc: 0.3763 - val_loss: 1.0971 - val_acc: 0.3167\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0847 - acc: 0.4481 - val_loss: 1.0968 - val_acc: 0.3000\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0930 - acc: 0.3903 - val_loss: 1.0981 - val_acc: 0.3167\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0937 - acc: 0.3686 - val_loss: 1.0959 - val_acc: 0.3333\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0951 - acc: 0.3215 - val_loss: 1.0957 - val_acc: 0.4167\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0850 - acc: 0.3748 - val_loss: 1.0953 - val_acc: 0.3833\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0914 - acc: 0.3570 - val_loss: 1.0959 - val_acc: 0.3167\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 16ms/step - loss: 1.1289 - acc: 0.3061 - val_loss: 1.0946 - val_acc: 0.3500\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1141 - acc: 0.3885 - val_loss: 1.0991 - val_acc: 0.3000\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1060 - acc: 0.3356 - val_loss: 1.1000 - val_acc: 0.3333\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1091 - acc: 0.3038 - val_loss: 1.0976 - val_acc: 0.3333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0973 - acc: 0.3483 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1169 - acc: 0.3028 - val_loss: 1.0962 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1002 - acc: 0.3237 - val_loss: 1.0956 - val_acc: 0.3333\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1010 - acc: 0.3197 - val_loss: 1.0976 - val_acc: 0.4500\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1034 - acc: 0.3480 - val_loss: 1.0946 - val_acc: 0.3667\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0997 - acc: 0.3409 - val_loss: 1.0963 - val_acc: 0.3333\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1039 - acc: 0.3276 - val_loss: 1.0985 - val_acc: 0.3333\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1075 - acc: 0.2966 - val_loss: 1.0958 - val_acc: 0.4167\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0949 - acc: 0.3563 - val_loss: 1.0978 - val_acc: 0.3333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1011 - acc: 0.3424 - val_loss: 1.0967 - val_acc: 0.3667\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1057 - acc: 0.3242 - val_loss: 1.0976 - val_acc: 0.4000\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1050 - acc: 0.3108 - val_loss: 1.0960 - val_acc: 0.3833\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0932 - acc: 0.3734 - val_loss: 1.0933 - val_acc: 0.3167\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0967 - acc: 0.3528 - val_loss: 1.0947 - val_acc: 0.3333\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1019 - acc: 0.3249 - val_loss: 1.0957 - val_acc: 0.2833\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0942 - acc: 0.3441 - val_loss: 1.0976 - val_acc: 0.2833\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0984 - acc: 0.3423 - val_loss: 1.0958 - val_acc: 0.3333\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0972 - acc: 0.3641 - val_loss: 1.0945 - val_acc: 0.3667\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0963 - acc: 0.3557 - val_loss: 1.0943 - val_acc: 0.3667\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0962 - acc: 0.3438 - val_loss: 1.0956 - val_acc: 0.3000\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0978 - acc: 0.3609 - val_loss: 1.0942 - val_acc: 0.5167\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0882 - acc: 0.4162 - val_loss: 1.0952 - val_acc: 0.3167\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0911 - acc: 0.3705 - val_loss: 1.0959 - val_acc: 0.3333\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1006 - acc: 0.3541 - val_loss: 1.0947 - val_acc: 0.3500\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0902 - acc: 0.3801 - val_loss: 1.0955 - val_acc: 0.3667\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0924 - acc: 0.3720 - val_loss: 1.0954 - val_acc: 0.3500\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 26ms/step - loss: 1.1449 - acc: 0.3339 - val_loss: 1.0992 - val_acc: 0.3333\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1228 - acc: 0.3013 - val_loss: 1.1006 - val_acc: 0.3333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0990 - acc: 0.3561 - val_loss: 1.0967 - val_acc: 0.3500\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1056 - acc: 0.3522 - val_loss: 1.1005 - val_acc: 0.3167\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1017 - acc: 0.3496 - val_loss: 1.0995 - val_acc: 0.3833\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1072 - acc: 0.3185 - val_loss: 1.1007 - val_acc: 0.2500\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0930 - acc: 0.4090 - val_loss: 1.0969 - val_acc: 0.3667\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1170 - acc: 0.3049 - val_loss: 1.0981 - val_acc: 0.3833\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0924 - acc: 0.3835 - val_loss: 1.0975 - val_acc: 0.3667\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1058 - acc: 0.3179 - val_loss: 1.0977 - val_acc: 0.4333\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0972 - acc: 0.3456 - val_loss: 1.0974 - val_acc: 0.3833\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0955 - acc: 0.3677 - val_loss: 1.0971 - val_acc: 0.4000\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1002 - acc: 0.3509 - val_loss: 1.0973 - val_acc: 0.3333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0961 - acc: 0.3546 - val_loss: 1.0975 - val_acc: 0.3667\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0932 - acc: 0.3500 - val_loss: 1.0973 - val_acc: 0.3667\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0918 - acc: 0.4005 - val_loss: 1.0966 - val_acc: 0.3833\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0925 - acc: 0.3716 - val_loss: 1.0975 - val_acc: 0.3333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1036 - acc: 0.3444 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0962 - acc: 0.3437 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0974 - acc: 0.3066 - val_loss: 1.0955 - val_acc: 0.4000\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0926 - acc: 0.3620 - val_loss: 1.0988 - val_acc: 0.3167\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1040 - acc: 0.3494 - val_loss: 1.0960 - val_acc: 0.4333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0958 - acc: 0.3503 - val_loss: 1.0984 - val_acc: 0.3667\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0956 - acc: 0.3985 - val_loss: 1.0969 - val_acc: 0.3667\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0971 - acc: 0.3077 - val_loss: 1.0971 - val_acc: 0.4167\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0964 - acc: 0.3796 - val_loss: 1.0984 - val_acc: 0.3500\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0959 - acc: 0.3606 - val_loss: 1.0974 - val_acc: 0.3500\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0952 - acc: 0.3727 - val_loss: 1.0969 - val_acc: 0.3667\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0944 - acc: 0.3354 - val_loss: 1.0975 - val_acc: 0.4000\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0891 - acc: 0.3683 - val_loss: 1.0979 - val_acc: 0.4000\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 24ms/step - loss: 1.1509 - acc: 0.3300 - val_loss: 1.1013 - val_acc: 0.3167\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1216 - acc: 0.3351 - val_loss: 1.1016 - val_acc: 0.3333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.1140 - acc: 0.3217 - val_loss: 1.0992 - val_acc: 0.3000\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0948 - acc: 0.3534 - val_loss: 1.0983 - val_acc: 0.2833\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1133 - acc: 0.2966 - val_loss: 1.0977 - val_acc: 0.3667\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0975 - acc: 0.3790 - val_loss: 1.0968 - val_acc: 0.4000\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1013 - acc: 0.3437 - val_loss: 1.0947 - val_acc: 0.4000\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1004 - acc: 0.3436 - val_loss: 1.0949 - val_acc: 0.3167\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.1024 - acc: 0.3306 - val_loss: 1.0957 - val_acc: 0.4333\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0996 - acc: 0.3201 - val_loss: 1.0959 - val_acc: 0.3833\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0952 - acc: 0.3517 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0992 - acc: 0.3337 - val_loss: 1.0970 - val_acc: 0.3667\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1009 - acc: 0.3002 - val_loss: 1.0981 - val_acc: 0.3167\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0962 - acc: 0.3733 - val_loss: 1.0971 - val_acc: 0.3333\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0982 - acc: 0.3429 - val_loss: 1.0967 - val_acc: 0.3500\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0935 - acc: 0.3266 - val_loss: 1.0939 - val_acc: 0.3833\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0946 - acc: 0.3479 - val_loss: 1.0981 - val_acc: 0.3667\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0982 - acc: 0.3671 - val_loss: 1.0964 - val_acc: 0.4500\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0967 - acc: 0.3463 - val_loss: 1.0950 - val_acc: 0.3833\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0921 - acc: 0.3721 - val_loss: 1.0948 - val_acc: 0.3500\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0962 - acc: 0.3403 - val_loss: 1.0948 - val_acc: 0.3667\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0943 - acc: 0.3597 - val_loss: 1.0952 - val_acc: 0.3500\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0907 - acc: 0.3509 - val_loss: 1.0960 - val_acc: 0.3333\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0932 - acc: 0.3598 - val_loss: 1.0928 - val_acc: 0.3500\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0959 - acc: 0.3374 - val_loss: 1.0943 - val_acc: 0.3667\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0898 - acc: 0.4045 - val_loss: 1.0927 - val_acc: 0.4000\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0904 - acc: 0.3849 - val_loss: 1.0909 - val_acc: 0.3833\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0866 - acc: 0.3583 - val_loss: 1.0864 - val_acc: 0.4667\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0792 - acc: 0.4034 - val_loss: 1.0930 - val_acc: 0.3667\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0828 - acc: 0.3867 - val_loss: 1.0882 - val_acc: 0.4167\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "bac_per_fold = []\n",
    "f1_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "for train, test in KFold(n_splits=5, shuffle=True).split(inputs, targets):\n",
    "    model_ex = models.Sequential()\n",
    "    model_ex.add(layers.Flatten())\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(64, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block5_conv1':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model_ex.compile(optimizer=optimizers.SGD(lr= 0.0030),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    with device('/GPU:0'):\n",
    "        history = model_ex.fit(train_features, train_labels, epochs=30, batch_size=batch_size, validation_data=(validation_features, validation_labels))\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    predctions = np.argmax(model_ex.predict(inputs[test], batch_size=batch_size,verbose=0), axis=-1)\n",
    "    scores = model_ex.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    acc_per_fold.append((scores[1] * 100))\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    rounded_labels=np.argmax(targets[test], axis=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(rounded_labels, predctions, labels=[0,1]).ravel()\n",
    "    bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp)))\n",
    "    f1 = (2*tp)/(2*tp+fp+fn)\n",
    "    bac_per_fold.append(bac) # balanced accuracy\n",
    "    f1_per_fold.append(f1) # F1\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.10784294605255126 - Accuracy: 0.3919999897480011%\n",
      ">       BAC: 0.5 - F1: 0.0%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.108652663230896 - Accuracy: 0.5120000243186951%\n",
      ">       BAC: 0.6237373737373737 - F1: 0.6285714285714286%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.10922437906265259 - Accuracy: 0.35199999809265137%\n",
      ">       BAC: 0.5303030303030303 - F1: 0.11428571428571428%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.10954463481903076 - Accuracy: 0.3840000033378601%\n",
      ">       BAC: 0.3333333333333333 - F1: 0.0%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.10869581699371338 - Accuracy: 0.4193548262119293%\n",
      ">       BAC: 0.5615079365079365 - F1: 0.6265060240963856%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.4118709683418274 (+- 5.447571117212694)\n",
      "> BAC: 0.5097763347763348 (+- 0.09727520930629359)\n",
      "> F1: 0.27387263339070567 (+- 0.2917677044956771)\n",
      "> Loss: 0.1087920880317688\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]/10} - Accuracy: {acc_per_fold[i]/100}%')\n",
    "    print(f'>       BAC: {bac_per_fold[i]} - F1: {f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)/100} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> BAC: {np.mean(bac_per_fold)} (+- {np.std(bac_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)/10}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "mylist = []\n",
    "for j in test_labels: mylist.append([int(i) for i in j])\n",
    "mylist = np.array(mylist)\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs_ex = model_ex.predict(test_features)\n",
    "preds_ex = probs_ex[:, 1]\n",
    "n_classes = 3\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(mylist[:, i], probs_ex[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "roc_aucAfter = []\n",
    "for i in range(3): roc_aucAfter.append(roc_auc[i])\n",
    "fprVgg16 = []\n",
    "tprVgg16 = []\n",
    "roc_aucVgg16 = []\n",
    "for i in range(3):\n",
    "    if roc_aucAfter[i] != max(roc_aucAfter) and roc_aucAfter[i] != min(roc_aucAfter):\n",
    "        fprVgg16 = fpr[i]\n",
    "        tprVgg16 = tpr[i]\n",
    "        roc_aucVgg16 = roc_auc[i]\n",
    "        \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(mylist.ravel(), probs_ex.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 210, 210, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 210, 210, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 210, 210, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 105, 105, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 105, 105, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 105, 105, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 52, 52, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 52, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 52, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 52, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 26, 26, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 26, 26, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "conv_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(210,210, 3))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\feras\\\\Bovine data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 21\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "Found 84 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 6, 6, 512))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    \n",
    "    if sample_count == 540:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1/255,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.2)\n",
    "    else:\n",
    "        train_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = train_datagen.flow_from_directory(directory,\n",
    "        target_size=(210,210),\n",
    "        batch_size = batch_size, \n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "train_features, train_labels = extract_features(train_dir, 540)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 60)\n",
    "test_features, test_labels = extract_features(test_dir, 84)\n",
    "##----------------------------------------------------------\n",
    "train_features = np.reshape(train_features, (540, 6* 6* 512))\n",
    "validation_features = np.reshape(validation_features, (60, 6* 6* 512))\n",
    "test_features = np.reshape(test_features, (84, 6* 6* 512))\n",
    "inputs = np.concatenate((train_features, test_features), axis=0)\n",
    "targets = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 19ms/step - loss: 1.1272 - acc: 0.2887 - val_loss: 1.1024 - val_acc: 0.4167\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1119 - acc: 0.3361 - val_loss: 1.1000 - val_acc: 0.3333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1300 - acc: 0.2798 - val_loss: 1.0951 - val_acc: 0.3500\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1017 - acc: 0.3448 - val_loss: 1.0992 - val_acc: 0.3500\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0962 - acc: 0.3756 - val_loss: 1.0968 - val_acc: 0.3667\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1066 - acc: 0.3277 - val_loss: 1.0994 - val_acc: 0.3000\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1001 - acc: 0.3620 - val_loss: 1.0987 - val_acc: 0.3500\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1017 - acc: 0.3425 - val_loss: 1.0970 - val_acc: 0.4000\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0993 - acc: 0.3570 - val_loss: 1.0981 - val_acc: 0.3333\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0973 - acc: 0.3606 - val_loss: 1.0967 - val_acc: 0.3667\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0964 - acc: 0.3871 - val_loss: 1.0975 - val_acc: 0.3500\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0975 - acc: 0.3433 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1066 - acc: 0.2982 - val_loss: 1.0979 - val_acc: 0.3667\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1044 - acc: 0.3362 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0972 - acc: 0.3643 - val_loss: 1.0980 - val_acc: 0.3667\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1004 - acc: 0.3471 - val_loss: 1.0992 - val_acc: 0.3500\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0959 - acc: 0.3699 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1044 - acc: 0.3218 - val_loss: 1.0995 - val_acc: 0.3333\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1044 - acc: 0.3627 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0982 - acc: 0.3401 - val_loss: 1.0982 - val_acc: 0.3333\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0933 - acc: 0.3912 - val_loss: 1.0973 - val_acc: 0.3667\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0968 - acc: 0.3567 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0981 - acc: 0.3330 - val_loss: 1.0990 - val_acc: 0.2833\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0951 - acc: 0.3491 - val_loss: 1.0977 - val_acc: 0.3667\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0954 - acc: 0.3372 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.0971 - acc: 0.3700 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1040 - acc: 0.2988 - val_loss: 1.0969 - val_acc: 0.3500\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1024 - acc: 0.3190 - val_loss: 1.0990 - val_acc: 0.3500\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0984 - acc: 0.3652 - val_loss: 1.0976 - val_acc: 0.3500\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0930 - acc: 0.3695 - val_loss: 1.0981 - val_acc: 0.3500\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 20ms/step - loss: 1.1090 - acc: 0.3463 - val_loss: 1.1019 - val_acc: 0.2167\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1449 - acc: 0.3052 - val_loss: 1.1015 - val_acc: 0.3333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1139 - acc: 0.3735 - val_loss: 1.1024 - val_acc: 0.3333\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1050 - acc: 0.3224 - val_loss: 1.0979 - val_acc: 0.3333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1015 - acc: 0.3695 - val_loss: 1.0972 - val_acc: 0.3833\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1096 - acc: 0.3285 - val_loss: 1.0987 - val_acc: 0.4167\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0874 - acc: 0.4006 - val_loss: 1.0971 - val_acc: 0.3333\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.1140 - acc: 0.3120 - val_loss: 1.0971 - val_acc: 0.3167\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0947 - acc: 0.3846 - val_loss: 1.0980 - val_acc: 0.3500\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0988 - acc: 0.3493 - val_loss: 1.0998 - val_acc: 0.3333\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1026 - acc: 0.3490 - val_loss: 1.0964 - val_acc: 0.3667\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1037 - acc: 0.3399 - val_loss: 1.0988 - val_acc: 0.3000\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0971 - acc: 0.3431 - val_loss: 1.0982 - val_acc: 0.3667\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0981 - acc: 0.3579 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1005 - acc: 0.3302 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0994 - acc: 0.3400 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0964 - acc: 0.3905 - val_loss: 1.0986 - val_acc: 0.3833\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1059 - acc: 0.3060 - val_loss: 1.0985 - val_acc: 0.3333\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0979 - acc: 0.3312 - val_loss: 1.0994 - val_acc: 0.3333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0982 - acc: 0.3685 - val_loss: 1.0992 - val_acc: 0.2833\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0963 - acc: 0.3729 - val_loss: 1.0968 - val_acc: 0.3333\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1006 - acc: 0.2923 - val_loss: 1.0979 - val_acc: 0.4000\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.1001 - acc: 0.3290 - val_loss: 1.0969 - val_acc: 0.3833\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0948 - acc: 0.3430 - val_loss: 1.0956 - val_acc: 0.3667\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0965 - acc: 0.3237 - val_loss: 1.0959 - val_acc: 0.3833\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.1010 - acc: 0.3449 - val_loss: 1.0976 - val_acc: 0.3500\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0988 - acc: 0.3329 - val_loss: 1.0967 - val_acc: 0.3833\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.0934 - acc: 0.3523 - val_loss: 1.0965 - val_acc: 0.3500\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0990 - acc: 0.3183 - val_loss: 1.0965 - val_acc: 0.3667\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0996 - acc: 0.3382 - val_loss: 1.0968 - val_acc: 0.3667\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 15ms/step - loss: 1.1578 - acc: 0.3059 - val_loss: 1.1031 - val_acc: 0.3333\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1205 - acc: 0.3145 - val_loss: 1.0967 - val_acc: 0.3333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1111 - acc: 0.3344 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1088 - acc: 0.3228 - val_loss: 1.0980 - val_acc: 0.3167\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0960 - acc: 0.3192 - val_loss: 1.0978 - val_acc: 0.4167\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1000 - acc: 0.3406 - val_loss: 1.0980 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0918 - acc: 0.4152 - val_loss: 1.0971 - val_acc: 0.4167\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1022 - acc: 0.3164 - val_loss: 1.0980 - val_acc: 0.3500\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0987 - acc: 0.3622 - val_loss: 1.0961 - val_acc: 0.3500\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1021 - acc: 0.2958 - val_loss: 1.0975 - val_acc: 0.3333\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0989 - acc: 0.3212 - val_loss: 1.0965 - val_acc: 0.3500\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0984 - acc: 0.3372 - val_loss: 1.0957 - val_acc: 0.4167\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0961 - acc: 0.3760 - val_loss: 1.0959 - val_acc: 0.3667\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0965 - acc: 0.3646 - val_loss: 1.0969 - val_acc: 0.3500\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0951 - acc: 0.3640 - val_loss: 1.0969 - val_acc: 0.3000\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0944 - acc: 0.3389 - val_loss: 1.0980 - val_acc: 0.3667\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0990 - acc: 0.3013 - val_loss: 1.0960 - val_acc: 0.4167\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0986 - acc: 0.3285 - val_loss: 1.0959 - val_acc: 0.3500\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1004 - acc: 0.2992 - val_loss: 1.0956 - val_acc: 0.4167\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0971 - acc: 0.3435 - val_loss: 1.0965 - val_acc: 0.3167\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0970 - acc: 0.3496 - val_loss: 1.0974 - val_acc: 0.3333\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0881 - acc: 0.4166 - val_loss: 1.0964 - val_acc: 0.3333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0950 - acc: 0.3523 - val_loss: 1.0954 - val_acc: 0.4000\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0915 - acc: 0.4011 - val_loss: 1.0952 - val_acc: 0.4167\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0977 - acc: 0.3417 - val_loss: 1.0955 - val_acc: 0.3833\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0967 - acc: 0.3369 - val_loss: 1.0949 - val_acc: 0.4500\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0943 - acc: 0.3834 - val_loss: 1.0955 - val_acc: 0.3833\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0943 - acc: 0.4092 - val_loss: 1.0964 - val_acc: 0.3667\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1030 - acc: 0.3191 - val_loss: 1.0973 - val_acc: 0.3667\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0981 - acc: 0.3314 - val_loss: 1.0957 - val_acc: 0.3667\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 18ms/step - loss: 1.1405 - acc: 0.2603 - val_loss: 1.0991 - val_acc: 0.3000\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1064 - acc: 0.3327 - val_loss: 1.0973 - val_acc: 0.3333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1047 - acc: 0.3613 - val_loss: 1.0975 - val_acc: 0.3333\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1303 - acc: 0.3057 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0958 - acc: 0.3461 - val_loss: 1.1013 - val_acc: 0.3000\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1138 - acc: 0.3086 - val_loss: 1.1004 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1138 - acc: 0.3076 - val_loss: 1.0984 - val_acc: 0.3167\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0972 - acc: 0.3577 - val_loss: 1.0999 - val_acc: 0.3000\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1059 - acc: 0.3204 - val_loss: 1.0983 - val_acc: 0.3167\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1027 - acc: 0.3261 - val_loss: 1.0986 - val_acc: 0.3167\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1051 - acc: 0.2803 - val_loss: 1.1012 - val_acc: 0.3167\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0965 - acc: 0.3820 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0976 - acc: 0.3363 - val_loss: 1.1016 - val_acc: 0.3333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1044 - acc: 0.3042 - val_loss: 1.1001 - val_acc: 0.3333\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0995 - acc: 0.3894 - val_loss: 1.0994 - val_acc: 0.3333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0961 - acc: 0.3458 - val_loss: 1.0996 - val_acc: 0.3333\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1087 - acc: 0.3098 - val_loss: 1.0986 - val_acc: 0.3833\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1009 - acc: 0.3011 - val_loss: 1.0994 - val_acc: 0.3333\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0984 - acc: 0.3179 - val_loss: 1.0985 - val_acc: 0.3333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0976 - acc: 0.3559 - val_loss: 1.0987 - val_acc: 0.3500\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0970 - acc: 0.3984 - val_loss: 1.0991 - val_acc: 0.2667\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0969 - acc: 0.3256 - val_loss: 1.0987 - val_acc: 0.3667\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0962 - acc: 0.3885 - val_loss: 1.0983 - val_acc: 0.4333\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1021 - acc: 0.3292 - val_loss: 1.1006 - val_acc: 0.2500\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1005 - acc: 0.3072 - val_loss: 1.0970 - val_acc: 0.4500\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0967 - acc: 0.3377 - val_loss: 1.0999 - val_acc: 0.3000\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0992 - acc: 0.3340 - val_loss: 1.0981 - val_acc: 0.3167\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1016 - acc: 0.3614 - val_loss: 1.0968 - val_acc: 0.4000\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1003 - acc: 0.3332 - val_loss: 1.0980 - val_acc: 0.3667\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0989 - acc: 0.3850 - val_loss: 1.0980 - val_acc: 0.3833\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 22ms/step - loss: 1.1194 - acc: 0.3576 - val_loss: 1.1035 - val_acc: 0.3333\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1114 - acc: 0.3396 - val_loss: 1.1000 - val_acc: 0.3333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1105 - acc: 0.3119 - val_loss: 1.1024 - val_acc: 0.2833\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1036 - acc: 0.3346 - val_loss: 1.0992 - val_acc: 0.3333\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1117 - acc: 0.2999 - val_loss: 1.0977 - val_acc: 0.4167\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0986 - acc: 0.3022 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1014 - acc: 0.3302 - val_loss: 1.0986 - val_acc: 0.3500\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0988 - acc: 0.3460 - val_loss: 1.0997 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1045 - acc: 0.3476 - val_loss: 1.0975 - val_acc: 0.3500\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1004 - acc: 0.3697 - val_loss: 1.0982 - val_acc: 0.3667\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1152 - acc: 0.2943 - val_loss: 1.0984 - val_acc: 0.3167\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1102 - acc: 0.3285 - val_loss: 1.0994 - val_acc: 0.2667\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1022 - acc: 0.3487 - val_loss: 1.0992 - val_acc: 0.2667\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0999 - acc: 0.3684 - val_loss: 1.0977 - val_acc: 0.3500\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1011 - acc: 0.3146 - val_loss: 1.0977 - val_acc: 0.3500\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0987 - acc: 0.3219 - val_loss: 1.0976 - val_acc: 0.3333\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1010 - acc: 0.3488 - val_loss: 1.0966 - val_acc: 0.4000\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1007 - acc: 0.3092 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0932 - acc: 0.3345 - val_loss: 1.0985 - val_acc: 0.3333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.0962 - acc: 0.3671 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1018 - acc: 0.3019 - val_loss: 1.0974 - val_acc: 0.4000\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1021 - acc: 0.3064 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0965 - acc: 0.3914 - val_loss: 1.0972 - val_acc: 0.3500\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0998 - acc: 0.3199 - val_loss: 1.0981 - val_acc: 0.3333\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0954 - acc: 0.3764 - val_loss: 1.0968 - val_acc: 0.4167\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0945 - acc: 0.3696 - val_loss: 1.0974 - val_acc: 0.3333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0997 - acc: 0.3334 - val_loss: 1.0981 - val_acc: 0.3333\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0978 - acc: 0.3678 - val_loss: 1.0970 - val_acc: 0.3833\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0974 - acc: 0.3225 - val_loss: 1.0969 - val_acc: 0.4000\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0986 - acc: 0.3816 - val_loss: 1.0973 - val_acc: 0.3500\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "bac_per_fold = []\n",
    "f1_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "for train, test in KFold(n_splits=5, shuffle=True).split(inputs, targets):\n",
    "    model_ex = models.Sequential()\n",
    "    model_ex.add(layers.Flatten())\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(64, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block5_conv1':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model_ex.compile(optimizer=optimizers.SGD(lr= 0.0030),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    with device('/GPU:0'):\n",
    "        history = model_ex.fit(train_features, train_labels, epochs=30, batch_size=batch_size, validation_data=(validation_features, validation_labels))\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    predctions = np.argmax(model_ex.predict(inputs[test], batch_size=batch_size,verbose=0), axis=-1)\n",
    "    scores = model_ex.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    acc_per_fold.append((scores[1] * 100))\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    rounded_labels=np.argmax(targets[test], axis=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(rounded_labels, predctions, labels=[0,1]).ravel()\n",
    "    bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp)))\n",
    "    f1 = (2*tp)/(2*tp+fp+fn)\n",
    "    bac_per_fold.append(bac) # balanced accuracy\n",
    "    f1_per_fold.append(f1) # F1\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.10918874740600586 - Accuracy: 0.4000000059604645%\n",
      ">       BAC: 0.5116279069767442 - F1: 0.045454545454545456%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.10940307378768921 - Accuracy: 0.36800000071525574%\n",
      ">       BAC: 0.5 - F1: 0.5357142857142857%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.10960487127304078 - Accuracy: 0.3440000116825104%\n",
      ">       BAC: 0.48460960960960964 - F1: 0.17391304347826086%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.10970747470855713 - Accuracy: 0.4000000059604645%\n",
      ">       BAC: 0.5756756756756757 - F1: 0.4482758620689655%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.10951774120330811 - Accuracy: 0.3709677457809448%\n",
      ">       BAC: 0.5 - F1: 0.72%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.376593554019928 (+- 2.1277038627273193)\n",
      "> BAC: 0.514382638452406 (+- 0.031826337463601945)\n",
      "> F1: 0.3846715473432115 (+- 0.24445527613422022)\n",
      "> Loss: 0.10948438167572021\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]/10} - Accuracy: {acc_per_fold[i]/100}%')\n",
    "    print(f'>       BAC: {bac_per_fold[i]} - F1: {f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)/100} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> BAC: {np.mean(bac_per_fold)} (+- {np.std(bac_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)/10}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "mylist = []\n",
    "for j in test_labels: mylist.append([int(i) for i in j])\n",
    "mylist = np.array(mylist)\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs_ex = model_ex.predict(test_features)\n",
    "preds_ex = probs_ex[:, 1]\n",
    "n_classes = 3\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(mylist[:, i], probs_ex[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "roc_aucAfter = []\n",
    "for i in range(3): roc_aucAfter.append(roc_auc[i])\n",
    "fprVgg19 = []\n",
    "tprVgg19 = []\n",
    "roc_aucVgg19 = []\n",
    "for i in range(3):\n",
    "    if roc_aucAfter[i] != max(roc_aucAfter) and roc_aucAfter[i] != min(roc_aucAfter):\n",
    "        fprVgg19 = fpr[i]\n",
    "        tprVgg19 = tpr[i]\n",
    "        roc_aucVgg19 = roc_auc[i]\n",
    "        \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(mylist.ravel(), probs_ex.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"xception\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 210, 210, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 104, 104, 32) 864         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 104, 104, 32) 128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 104, 104, 32) 0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 102, 102, 64) 18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 102, 102, 64) 256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 102, 102, 64) 0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 102, 102, 128 8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 102, 102, 128 512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 102, 102, 128 0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 102, 102, 128 17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 102, 102, 128 512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 51, 51, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 51, 51, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 51, 51, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 51, 51, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 51, 51, 128)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 51, 51, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 51, 51, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 51, 51, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 51, 51, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 51, 51, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 26, 26, 256)  32768       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 26, 26, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 26, 26, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 26, 26, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 26, 26, 256)  0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 26, 26, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 26, 26, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 26, 26, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 26, 26, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 26, 26, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 13, 13, 728)  186368      add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 13, 13, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 13, 13, 728)  2912        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 13, 13, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 13, 13, 728)  0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 13, 13, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 13, 13, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 13, 13, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 13, 13, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 13, 13, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 13, 13, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 13, 13, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 13, 13, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 13, 13, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 13, 13, 728)  0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 13, 13, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 13, 13, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 13, 13, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 13, 13, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 13, 13, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 13, 13, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 13, 13, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 13, 13, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 13, 13, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 13, 13, 728)  0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 13, 13, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 13, 13, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 13, 13, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 13, 13, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 13, 13, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 13, 13, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 13, 13, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 13, 13, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 13, 13, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 13, 13, 728)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 13, 13, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 13, 13, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 13, 13, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 13, 13, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 13, 13, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 13, 13, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 13, 13, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 13, 13, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 13, 13, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 13, 13, 728)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 13, 13, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 13, 13, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 13, 13, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 13, 13, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 13, 13, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 13, 13, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 13, 13, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 13, 13, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 13, 13, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 13, 13, 728)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 13, 13, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 13, 13, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 13, 13, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 13, 13, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 13, 13, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 13, 13, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 13, 13, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 13, 13, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 13, 13, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 13, 13, 728)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 13, 13, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 13, 13, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 13, 13, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 13, 13, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 13, 13, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 13, 13, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 13, 13, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 13, 13, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 13, 13, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 13, 13, 728)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 13, 13, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 13, 13, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 13, 13, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 13, 13, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 13, 13, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 13, 13, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 13, 13, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 13, 13, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 13, 13, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 13, 13, 728)  0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 13, 13, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 13, 13, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 13, 13, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 13, 13, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 13, 13, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 7, 7, 1024)   745472      add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 7, 7, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 7, 7, 1024)   4096        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 7, 7, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 7, 7, 1536)   1582080     add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 7, 7, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 7, 7, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 7, 7, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 7, 7, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 7, 7, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 20,806,952\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "conv_base = Xception(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(210,210, 3))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\feras\\\\Bovine data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 21\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "Found 84 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 2048))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    \n",
    "    if sample_count == 540:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1/255,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.2)\n",
    "    else:\n",
    "        train_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = train_datagen.flow_from_directory(directory,\n",
    "        target_size=(210,210),\n",
    "        batch_size = batch_size, \n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "train_features, train_labels = extract_features(train_dir, 540)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 60)\n",
    "test_features, test_labels = extract_features(test_dir, 84)\n",
    "##----------------------------------------------------------\n",
    "train_features = np.reshape(train_features, (540, 7* 7* 2048))\n",
    "validation_features = np.reshape(validation_features, (60, 7* 7* 2048))\n",
    "test_features = np.reshape(test_features, (84, 7* 7* 2048))\n",
    "inputs = np.concatenate((train_features, test_features), axis=0)\n",
    "targets = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 44ms/step - loss: 1.1241 - acc: 0.3550 - val_loss: 1.0789 - val_acc: 0.3333\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.1049 - acc: 0.3759 - val_loss: 1.0860 - val_acc: 0.4000\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0807 - acc: 0.3752 - val_loss: 1.0580 - val_acc: 0.4500\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 1.0711 - acc: 0.4070 - val_loss: 1.0497 - val_acc: 0.4500\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 1.0440 - acc: 0.4474 - val_loss: 1.0310 - val_acc: 0.5000\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 1.0630 - acc: 0.4203 - val_loss: 1.0333 - val_acc: 0.4500\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 1.0454 - acc: 0.4227 - val_loss: 1.0221 - val_acc: 0.5500\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 1.0269 - acc: 0.4419 - val_loss: 1.0158 - val_acc: 0.4500\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 1.0324 - acc: 0.4593 - val_loss: 1.0256 - val_acc: 0.4500\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 1.0084 - acc: 0.4898 - val_loss: 1.0164 - val_acc: 0.5000\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 0.9705 - acc: 0.4994 - val_loss: 1.0145 - val_acc: 0.4667\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 0.9635 - acc: 0.5294 - val_loss: 1.0143 - val_acc: 0.4667\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9575 - acc: 0.5105 - val_loss: 1.0117 - val_acc: 0.5333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9816 - acc: 0.4804 - val_loss: 1.0113 - val_acc: 0.5167\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9552 - acc: 0.5028 - val_loss: 1.0101 - val_acc: 0.4667\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9752 - acc: 0.5251 - val_loss: 1.0074 - val_acc: 0.4500\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9492 - acc: 0.5080 - val_loss: 1.0105 - val_acc: 0.4667\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 0.9310 - acc: 0.5426 - val_loss: 1.0048 - val_acc: 0.4833\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.8952 - acc: 0.5458 - val_loss: 1.0038 - val_acc: 0.5000\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 0.9511 - acc: 0.5127 - val_loss: 1.0027 - val_acc: 0.4833\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9042 - acc: 0.5559 - val_loss: 1.0179 - val_acc: 0.4667\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 0.8929 - acc: 0.5872 - val_loss: 1.0247 - val_acc: 0.4667\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 0.8592 - acc: 0.5910 - val_loss: 1.0123 - val_acc: 0.5000\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 0.8686 - acc: 0.5794 - val_loss: 1.0061 - val_acc: 0.4500\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8182 - acc: 0.6134 - val_loss: 1.0153 - val_acc: 0.4833\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8442 - acc: 0.6019 - val_loss: 1.0121 - val_acc: 0.4500\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.8121 - acc: 0.6166 - val_loss: 1.0153 - val_acc: 0.5667\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.8275 - acc: 0.5914 - val_loss: 0.9945 - val_acc: 0.4833\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8388 - acc: 0.5685 - val_loss: 1.0110 - val_acc: 0.4667\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.7762 - acc: 0.6264 - val_loss: 1.0402 - val_acc: 0.4667\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 48ms/step - loss: 1.1283 - acc: 0.3452 - val_loss: 1.0932 - val_acc: 0.4000\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 1.0849 - acc: 0.3935 - val_loss: 1.0865 - val_acc: 0.4667\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0959 - acc: 0.4113 - val_loss: 1.0759 - val_acc: 0.4500\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 1.0994 - acc: 0.3628 - val_loss: 1.0604 - val_acc: 0.5000\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 1.0601 - acc: 0.4536 - val_loss: 1.0628 - val_acc: 0.4500\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 1.0622 - acc: 0.4664 - val_loss: 1.0543 - val_acc: 0.4667\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 1.0422 - acc: 0.4564 - val_loss: 1.0546 - val_acc: 0.4667\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 1.0421 - acc: 0.4510 - val_loss: 1.0319 - val_acc: 0.4833\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 1.0272 - acc: 0.4306 - val_loss: 1.0348 - val_acc: 0.4667\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 0.9941 - acc: 0.4458 - val_loss: 1.0323 - val_acc: 0.4833\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 1.0055 - acc: 0.4489 - val_loss: 1.0181 - val_acc: 0.4833\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 0.9854 - acc: 0.5051 - val_loss: 1.0199 - val_acc: 0.5167\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 0.9680 - acc: 0.5296 - val_loss: 1.0129 - val_acc: 0.4833\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 0.9463 - acc: 0.5362 - val_loss: 1.0229 - val_acc: 0.4667\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9765 - acc: 0.4859 - val_loss: 1.0158 - val_acc: 0.5000\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9453 - acc: 0.5347 - val_loss: 1.0079 - val_acc: 0.5000\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9552 - acc: 0.5309 - val_loss: 1.0078 - val_acc: 0.5333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9331 - acc: 0.5260 - val_loss: 1.0258 - val_acc: 0.5167\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 0.9109 - acc: 0.5604 - val_loss: 1.0194 - val_acc: 0.4833\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9070 - acc: 0.5656 - val_loss: 1.0201 - val_acc: 0.4500\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.8666 - acc: 0.5923 - val_loss: 1.0074 - val_acc: 0.5000\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.8510 - acc: 0.5695 - val_loss: 1.0217 - val_acc: 0.5000\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9018 - acc: 0.5588 - val_loss: 1.0127 - val_acc: 0.5667\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 0.8397 - acc: 0.5848 - val_loss: 1.0088 - val_acc: 0.4500\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 0.8662 - acc: 0.5821 - val_loss: 1.0006 - val_acc: 0.5000\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 0.8504 - acc: 0.5995 - val_loss: 1.0123 - val_acc: 0.5000\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 0.8207 - acc: 0.6104 - val_loss: 1.0225 - val_acc: 0.5500\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8533 - acc: 0.6117 - val_loss: 1.0237 - val_acc: 0.4833\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8335 - acc: 0.5930 - val_loss: 1.0185 - val_acc: 0.5500\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.7920 - acc: 0.6241 - val_loss: 1.0219 - val_acc: 0.5333\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 37ms/step - loss: 1.1248 - acc: 0.3397 - val_loss: 1.0876 - val_acc: 0.3500\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.1046 - acc: 0.3814 - val_loss: 1.0780 - val_acc: 0.4500\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0843 - acc: 0.3860 - val_loss: 1.0711 - val_acc: 0.4000\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0622 - acc: 0.4264 - val_loss: 1.0730 - val_acc: 0.4167\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0709 - acc: 0.4164 - val_loss: 1.0516 - val_acc: 0.4833\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0620 - acc: 0.3907 - val_loss: 1.0585 - val_acc: 0.4167\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0637 - acc: 0.3957 - val_loss: 1.0407 - val_acc: 0.4167\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 1.0091 - acc: 0.4468 - val_loss: 1.0353 - val_acc: 0.4667\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9911 - acc: 0.4846 - val_loss: 1.0297 - val_acc: 0.4167\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0176 - acc: 0.4759 - val_loss: 1.0209 - val_acc: 0.4500\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0102 - acc: 0.4898 - val_loss: 1.0192 - val_acc: 0.4667\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9842 - acc: 0.4889 - val_loss: 1.0317 - val_acc: 0.4333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 0.9995 - acc: 0.5047 - val_loss: 1.0126 - val_acc: 0.4667\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 0.9741 - acc: 0.4969 - val_loss: 1.0080 - val_acc: 0.4667\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 1.0138 - acc: 0.4761 - val_loss: 1.0096 - val_acc: 0.4833\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9279 - acc: 0.5362 - val_loss: 1.0160 - val_acc: 0.4833\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9400 - acc: 0.5447 - val_loss: 1.0072 - val_acc: 0.4667\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9271 - acc: 0.5567 - val_loss: 1.0042 - val_acc: 0.4667\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9002 - acc: 0.5502 - val_loss: 1.0236 - val_acc: 0.4500\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9002 - acc: 0.5491 - val_loss: 1.0001 - val_acc: 0.4833\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8992 - acc: 0.5510 - val_loss: 0.9997 - val_acc: 0.4667\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.9120 - acc: 0.5370 - val_loss: 1.0284 - val_acc: 0.5000\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8690 - acc: 0.5879 - val_loss: 1.0062 - val_acc: 0.4833\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8888 - acc: 0.5167 - val_loss: 1.0221 - val_acc: 0.4833\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8163 - acc: 0.5917 - val_loss: 0.9910 - val_acc: 0.5167\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8588 - acc: 0.5822 - val_loss: 1.0082 - val_acc: 0.4833\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8365 - acc: 0.5840 - val_loss: 1.0140 - val_acc: 0.4833\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8399 - acc: 0.5969 - val_loss: 1.0177 - val_acc: 0.4667\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.8090 - acc: 0.5886 - val_loss: 1.0049 - val_acc: 0.4667\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.7666 - acc: 0.6455 - val_loss: 1.0295 - val_acc: 0.4833\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 40ms/step - loss: 1.1246 - acc: 0.2848 - val_loss: 1.0798 - val_acc: 0.4167\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0773 - acc: 0.4150 - val_loss: 1.0583 - val_acc: 0.4333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0678 - acc: 0.4243 - val_loss: 1.0273 - val_acc: 0.4500\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0062 - acc: 0.4784 - val_loss: 1.0280 - val_acc: 0.4833\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0392 - acc: 0.4469 - val_loss: 1.0151 - val_acc: 0.4667\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0033 - acc: 0.4584 - val_loss: 1.0022 - val_acc: 0.4500\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9809 - acc: 0.5069 - val_loss: 1.0110 - val_acc: 0.4833\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9993 - acc: 0.4630 - val_loss: 1.0129 - val_acc: 0.4667\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0125 - acc: 0.4605 - val_loss: 1.0156 - val_acc: 0.4833\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.9504 - acc: 0.4941 - val_loss: 1.0045 - val_acc: 0.4500\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.9674 - acc: 0.4775 - val_loss: 1.0104 - val_acc: 0.4833\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.9659 - acc: 0.4892 - val_loss: 1.0118 - val_acc: 0.4833\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.9105 - acc: 0.5530 - val_loss: 1.0186 - val_acc: 0.4500\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8811 - acc: 0.6039 - val_loss: 1.0045 - val_acc: 0.4667\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.9007 - acc: 0.5369 - val_loss: 0.9967 - val_acc: 0.4667\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.9249 - acc: 0.5111 - val_loss: 1.0044 - val_acc: 0.4500\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9035 - acc: 0.5651 - val_loss: 0.9969 - val_acc: 0.4667\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9165 - acc: 0.5389 - val_loss: 0.9987 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8365 - acc: 0.5938 - val_loss: 1.0144 - val_acc: 0.4667\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8890 - acc: 0.5704 - val_loss: 0.9920 - val_acc: 0.5167\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8721 - acc: 0.5942 - val_loss: 1.0300 - val_acc: 0.5500\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8687 - acc: 0.5995 - val_loss: 1.0234 - val_acc: 0.4833\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8475 - acc: 0.6019 - val_loss: 1.0152 - val_acc: 0.5333\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8241 - acc: 0.5777 - val_loss: 0.9885 - val_acc: 0.5000\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.7936 - acc: 0.6208 - val_loss: 1.0233 - val_acc: 0.4667\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8001 - acc: 0.6301 - val_loss: 1.0068 - val_acc: 0.4667\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.7847 - acc: 0.6001 - val_loss: 0.9994 - val_acc: 0.5167\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.7961 - acc: 0.6258 - val_loss: 0.9856 - val_acc: 0.5167\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8125 - acc: 0.6021 - val_loss: 1.0535 - val_acc: 0.4833\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.7520 - acc: 0.6726 - val_loss: 0.9923 - val_acc: 0.5167\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 39ms/step - loss: 1.1086 - acc: 0.3431 - val_loss: 1.0930 - val_acc: 0.3333\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.1015 - acc: 0.3496 - val_loss: 1.0836 - val_acc: 0.4667\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0925 - acc: 0.3756 - val_loss: 1.0734 - val_acc: 0.4333\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0722 - acc: 0.4172 - val_loss: 1.0595 - val_acc: 0.5000\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0374 - acc: 0.4943 - val_loss: 1.0499 - val_acc: 0.5167\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0544 - acc: 0.4395 - val_loss: 1.0456 - val_acc: 0.5167\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0222 - acc: 0.4795 - val_loss: 1.0414 - val_acc: 0.5167\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0475 - acc: 0.4304 - val_loss: 1.0365 - val_acc: 0.4833\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0319 - acc: 0.4666 - val_loss: 1.0287 - val_acc: 0.4667\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9897 - acc: 0.4811 - val_loss: 1.0433 - val_acc: 0.4500\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9964 - acc: 0.4734 - val_loss: 1.0496 - val_acc: 0.4500\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9981 - acc: 0.5231 - val_loss: 1.0206 - val_acc: 0.4667\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9508 - acc: 0.5602 - val_loss: 1.0161 - val_acc: 0.4667\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9630 - acc: 0.5378 - val_loss: 1.0308 - val_acc: 0.4833\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9338 - acc: 0.5556 - val_loss: 1.0388 - val_acc: 0.4667\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9475 - acc: 0.5239 - val_loss: 1.0278 - val_acc: 0.4667\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.9593 - acc: 0.4885 - val_loss: 1.0338 - val_acc: 0.4833\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.9161 - acc: 0.5368 - val_loss: 1.0163 - val_acc: 0.5167\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9119 - acc: 0.5567 - val_loss: 1.0322 - val_acc: 0.4833\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8966 - acc: 0.5697 - val_loss: 1.0133 - val_acc: 0.4667\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9153 - acc: 0.5492 - val_loss: 1.0146 - val_acc: 0.4667\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8951 - acc: 0.5375 - val_loss: 1.0083 - val_acc: 0.5167\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.8460 - acc: 0.5874 - val_loss: 1.0461 - val_acc: 0.4833\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8635 - acc: 0.5687 - val_loss: 1.0167 - val_acc: 0.4833\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8747 - acc: 0.5890 - val_loss: 0.9860 - val_acc: 0.5167\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.8611 - acc: 0.6141 - val_loss: 1.0179 - val_acc: 0.4667\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.8345 - acc: 0.6481 - val_loss: 1.0309 - val_acc: 0.5000\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8396 - acc: 0.5842 - val_loss: 1.0921 - val_acc: 0.4667\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8366 - acc: 0.6104 - val_loss: 1.0508 - val_acc: 0.4500\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.7810 - acc: 0.6103 - val_loss: 0.9970 - val_acc: 0.5333\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "bac_per_fold = []\n",
    "f1_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "for train, test in KFold(n_splits=5, shuffle=True).split(inputs, targets):\n",
    "    model_ex = models.Sequential()\n",
    "    model_ex.add(layers.Flatten())\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(64, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block5_conv1':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model_ex.compile(optimizer=optimizers.SGD(lr= 0.0030),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    with device('/GPU:0'):\n",
    "        history = model_ex.fit(train_features, train_labels, epochs=30, batch_size=batch_size, validation_data=(validation_features, validation_labels))\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    predctions = np.argmax(model_ex.predict(inputs[test], batch_size=batch_size,verbose=0), axis=-1)\n",
    "    scores = model_ex.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    acc_per_fold.append((scores[1] * 100))\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    rounded_labels=np.argmax(targets[test], axis=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(rounded_labels, predctions, labels=[0,1]).ravel()\n",
    "    bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp)))\n",
    "    f1 = (2*tp)/(2*tp+fp+fn)\n",
    "    bac_per_fold.append(bac) # balanced accuracy\n",
    "    f1_per_fold.append(f1) # F1\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.07245122194290161 - Accuracy: 0.6800000071525574%\n",
      ">       BAC: 0.6875 - F1: 0.6197183098591549%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.06993256211280822 - Accuracy: 0.6880000233650208%\n",
      ">       BAC: 0.6416666666666666 - F1: 0.47368421052631576%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.07642709612846374 - Accuracy: 0.6959999799728394%\n",
      ">       BAC: 0.7504480286738351 - F1: 0.746268656716418%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.06814013719558716 - Accuracy: 0.7279999852180481%\n",
      ">       BAC: 0.7178861788617886 - F1: 0.64%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.07274333834648132 - Accuracy: 0.6935483813285828%\n",
      ">       BAC: 0.8260869565217391 - F1: 0.7894736842105263%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.6971096754074096 (+- 1.6394218418668938)\n",
      "> BAC: 0.724717566144806 (+- 0.06207588419576547)\n",
      "> F1: 0.653828972262483 (+- 0.1102283956935068)\n",
      "> Loss: 0.07193887114524841\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]/10} - Accuracy: {acc_per_fold[i]/100}%')\n",
    "    print(f'>       BAC: {bac_per_fold[i]} - F1: {f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)/100} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> BAC: {np.mean(bac_per_fold)} (+- {np.std(bac_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)/10}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "mylist = []\n",
    "for j in test_labels: mylist.append([int(i) for i in j])\n",
    "mylist = np.array(mylist)\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs_ex = model_ex.predict(test_features)\n",
    "preds_ex = probs_ex[:, 1]\n",
    "n_classes = 3\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(mylist[:, i], probs_ex[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "roc_aucAfter = []\n",
    "for i in range(3): roc_aucAfter.append(roc_auc[i])\n",
    "fprXception = []\n",
    "tprXception = []\n",
    "roc_aucXception = []\n",
    "for i in range(3):\n",
    "    if roc_aucAfter[i] != max(roc_aucAfter) and roc_aucAfter[i] != min(roc_aucAfter):\n",
    "        fprXception = fpr[i]\n",
    "        tprXception = tpr[i]\n",
    "        roc_aucXception = roc_auc[i]\n",
    "        \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(mylist.ravel(), probs_ex.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"densenet121\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 210, 210, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 216, 216, 3)  0           input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 105, 105, 64) 9408        zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 105, 105, 64) 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 105, 105, 64) 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 107, 107, 64) 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 53, 53, 64)   0           zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 53, 53, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 53, 53, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 53, 53, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 53, 53, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 53, 53, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 53, 53, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 53, 53, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 53, 53, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 53, 53, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 53, 53, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 53, 53, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 53, 53, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 53, 53, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 53, 53, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 53, 53, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 53, 53, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 53, 53, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 53, 53, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 53, 53, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 53, 53, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 53, 53, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 53, 53, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 53, 53, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 53, 53, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 53, 53, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 53, 53, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 53, 53, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 53, 53, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 53, 53, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 53, 53, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 53, 53, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 53, 53, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 26, 26, 128)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 26, 26, 128)  512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 26, 26, 128)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 26, 26, 128)  16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 26, 26, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 26, 26, 160)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 26, 26, 160)  640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 26, 26, 160)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 26, 26, 128)  20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 26, 26, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 26, 26, 192)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 26, 26, 192)  768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 26, 26, 192)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 26, 26, 128)  24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 26, 26, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 26, 26, 224)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 26, 26, 224)  896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 26, 26, 224)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 26, 26, 128)  28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 26, 26, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 26, 26, 256)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 26, 26, 256)  1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 26, 26, 256)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 26, 26, 128)  32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 26, 26, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 26, 26, 288)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 26, 26, 288)  1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 26, 26, 288)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 26, 26, 128)  36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 26, 26, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 26, 26, 320)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 26, 26, 320)  1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 26, 26, 320)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 26, 26, 128)  40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 26, 26, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 26, 26, 352)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 26, 26, 352)  1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 26, 26, 352)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 26, 26, 128)  45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 26, 26, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 26, 26, 384)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 26, 26, 384)  1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 26, 26, 384)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 26, 26, 128)  49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 26, 26, 128)  0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 26, 26, 416)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 26, 26, 416)  1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 26, 26, 416)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 26, 26, 128)  53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 26, 26, 128)  512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 26, 26, 128)  0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 26, 26, 32)   36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 26, 26, 448)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 26, 26, 448)  1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 26, 26, 448)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 26, 26, 128)  57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 26, 26, 128)  512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 26, 26, 128)  0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 26, 26, 32)   36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 26, 26, 480)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 26, 26, 480)  1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 26, 26, 480)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 26, 26, 128)  61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 26, 26, 128)  512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 26, 26, 128)  0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 26, 26, 32)   36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 26, 26, 512)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 26, 26, 512)  2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 26, 26, 512)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 26, 26, 256)  131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 13, 13, 256)  0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 13, 13, 256)  1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 13, 13, 256)  0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 13, 13, 128)  32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 13, 13, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 13, 13, 288)  0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 13, 13, 288)  1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 13, 13, 288)  0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 13, 13, 128)  36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 13, 13, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 13, 13, 320)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 13, 13, 320)  1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 13, 13, 320)  0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 13, 13, 128)  40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 13, 13, 128)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 13, 13, 352)  0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 13, 13, 352)  1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 13, 13, 352)  0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 13, 13, 128)  45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 13, 13, 128)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 13, 13, 384)  0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 13, 13, 384)  1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 13, 13, 384)  0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 13, 13, 128)  49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 13, 13, 128)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 13, 13, 416)  0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 13, 13, 416)  1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 13, 13, 416)  0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 13, 13, 128)  53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 13, 13, 128)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 13, 13, 448)  0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 13, 13, 448)  1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 13, 13, 448)  0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 13, 13, 128)  57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 13, 13, 128)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 13, 13, 480)  0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 13, 13, 480)  1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 13, 13, 480)  0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 13, 13, 128)  61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 13, 13, 128)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 13, 13, 512)  0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 13, 13, 512)  2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 13, 13, 512)  0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 13, 13, 128)  65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 13, 13, 128)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 13, 13, 544)  0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 13, 13, 544)  2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 13, 13, 544)  0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 13, 13, 128)  69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 13, 13, 576)  0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 13, 13, 576)  2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 13, 13, 576)  0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 13, 13, 128)  73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 13, 13, 608)  0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 13, 13, 608)  2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 13, 13, 608)  0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 13, 13, 128)  77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 13, 13, 640)  0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 13, 13, 640)  2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 13, 13, 640)  0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 13, 13, 128)  81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 13, 13, 672)  0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 13, 13, 672)  2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 13, 13, 672)  0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 13, 13, 128)  86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 13, 13, 704)  0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 13, 13, 704)  2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 13, 13, 704)  0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 13, 13, 128)  90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 13, 13, 736)  0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 13, 13, 736)  2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 13, 13, 736)  0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 13, 13, 128)  94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 13, 13, 768)  0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 13, 13, 768)  3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 13, 13, 768)  0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 13, 13, 128)  98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 13, 13, 800)  0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 13, 13, 800)  3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 13, 13, 800)  0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 13, 13, 128)  102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 13, 13, 832)  0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 13, 13, 832)  3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 13, 13, 832)  0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 13, 13, 128)  106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 13, 13, 864)  0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 13, 13, 864)  3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 13, 13, 864)  0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 13, 13, 128)  110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 13, 13, 896)  0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 13, 13, 896)  3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 13, 13, 896)  0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 13, 13, 128)  114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 13, 13, 928)  0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 13, 13, 928)  3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 13, 13, 928)  0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 13, 13, 128)  118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 13, 13, 960)  0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 13, 13, 960)  3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 13, 13, 960)  0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 13, 13, 128)  122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 13, 13, 992)  0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 13, 13, 992)  3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 13, 13, 992)  0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 13, 13, 128)  126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 13, 13, 1024) 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 13, 13, 1024) 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 13, 13, 1024) 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 13, 13, 512)  524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 6, 6, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 6, 6, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 6, 6, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 6, 6, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 6, 6, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 6, 6, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 6, 6, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 6, 6, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 6, 6, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 6, 6, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 6, 6, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 6, 6, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 6, 6, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 6, 6, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 6, 6, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 6, 6, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 6, 6, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 6, 6, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 6, 6, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 6, 6, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 6, 6, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 6, 6, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 6, 6, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 6, 6, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 6, 6, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 6, 6, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 6, 6, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 6, 6, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 6, 6, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 6, 6, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 6, 6, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 6, 6, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 6, 6, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 6, 6, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 6, 6, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 6, 6, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 6, 6, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 6, 6, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 6, 6, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 6, 6, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 6, 6, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 6, 6, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 6, 6, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 6, 6, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 6, 6, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 6, 6, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 6, 6, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 6, 6, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 6, 6, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 6, 6, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 6, 6, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 6, 6, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 6, 6, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 6, 6, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 6, 6, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 6, 6, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 6, 6, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 6, 6, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 6, 6, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 6, 6, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, 6, 6, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 6, 6, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 6, 6, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 6, 6, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 6, 6, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 6, 6, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 6, 6, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 6, 6, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 6, 6, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 6, 6, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 6, 6, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 6, 6, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 6, 6, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 6, 6, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 6, 6, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 6, 6, 1024)   0           bn[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 7,037,504\n",
      "Trainable params: 6,953,856\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "conv_base = DenseNet121(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(210,210, 3))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\feras\\\\Bovine data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 21\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "Found 84 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 6, 6, 1024))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    \n",
    "    if sample_count == 540:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1/255,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.2)\n",
    "    else:\n",
    "        train_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = train_datagen.flow_from_directory(directory,\n",
    "        target_size=(210,210),\n",
    "        batch_size = batch_size, \n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "train_features, train_labels = extract_features(train_dir, 540)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 60)\n",
    "test_features, test_labels = extract_features(test_dir, 84)\n",
    "##----------------------------------------------------------\n",
    "train_features = np.reshape(train_features, (540, 6* 6* 1024))\n",
    "validation_features = np.reshape(validation_features, (60, 6* 6* 1024))\n",
    "test_features = np.reshape(test_features, (84, 6* 6* 1024))\n",
    "inputs = np.concatenate((train_features, test_features), axis=0)\n",
    "targets = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 25ms/step - loss: 1.2430 - acc: 0.3070 - val_loss: 1.0959 - val_acc: 0.3500\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.1467 - acc: 0.3230 - val_loss: 1.0954 - val_acc: 0.3833\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.1298 - acc: 0.3755 - val_loss: 1.1009 - val_acc: 0.3333\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.1153 - acc: 0.3432 - val_loss: 1.0919 - val_acc: 0.3167\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0989 - acc: 0.3110 - val_loss: 1.0881 - val_acc: 0.4000\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0902 - acc: 0.4036 - val_loss: 1.0787 - val_acc: 0.3833\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0672 - acc: 0.3957 - val_loss: 1.0582 - val_acc: 0.4833\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0474 - acc: 0.4483 - val_loss: 1.0629 - val_acc: 0.4667\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0462 - acc: 0.4014 - val_loss: 1.0595 - val_acc: 0.4333\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0427 - acc: 0.4088 - val_loss: 1.0500 - val_acc: 0.3667\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0369 - acc: 0.4363 - val_loss: 1.0263 - val_acc: 0.5000\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0012 - acc: 0.4806 - val_loss: 1.0483 - val_acc: 0.4667\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0157 - acc: 0.4452 - val_loss: 1.0421 - val_acc: 0.4167\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0096 - acc: 0.4597 - val_loss: 1.0504 - val_acc: 0.4000\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9901 - acc: 0.5000 - val_loss: 1.0107 - val_acc: 0.5000\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9673 - acc: 0.5225 - val_loss: 1.0183 - val_acc: 0.4500\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0025 - acc: 0.4535 - val_loss: 1.0284 - val_acc: 0.4167\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.9656 - acc: 0.5005 - val_loss: 1.0140 - val_acc: 0.5167\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9611 - acc: 0.5310 - val_loss: 1.0189 - val_acc: 0.4333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9793 - acc: 0.5117 - val_loss: 1.0003 - val_acc: 0.5000\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9210 - acc: 0.5615 - val_loss: 1.0337 - val_acc: 0.4333\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9565 - acc: 0.5235 - val_loss: 0.9872 - val_acc: 0.4833\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8625 - acc: 0.6081 - val_loss: 1.0186 - val_acc: 0.4000\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9292 - acc: 0.5163 - val_loss: 0.9882 - val_acc: 0.4667\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9209 - acc: 0.5009 - val_loss: 0.9666 - val_acc: 0.5000\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9102 - acc: 0.5762 - val_loss: 0.9768 - val_acc: 0.5000\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8533 - acc: 0.5896 - val_loss: 0.9542 - val_acc: 0.5667\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8837 - acc: 0.5631 - val_loss: 0.9345 - val_acc: 0.6000\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8711 - acc: 0.5674 - val_loss: 0.9624 - val_acc: 0.4167\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8950 - acc: 0.5276 - val_loss: 0.9708 - val_acc: 0.5000\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 22ms/step - loss: 1.2138 - acc: 0.3351 - val_loss: 1.1121 - val_acc: 0.3000\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.1201 - acc: 0.3741 - val_loss: 1.0813 - val_acc: 0.3667\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.1467 - acc: 0.3599 - val_loss: 1.1091 - val_acc: 0.4000\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.1019 - acc: 0.3910 - val_loss: 1.0885 - val_acc: 0.3500\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0664 - acc: 0.4672 - val_loss: 1.0848 - val_acc: 0.4500\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0930 - acc: 0.3791 - val_loss: 1.0971 - val_acc: 0.4000\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.0777 - acc: 0.3710 - val_loss: 1.0845 - val_acc: 0.4000\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0808 - acc: 0.4119 - val_loss: 1.0788 - val_acc: 0.4333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0460 - acc: 0.4556 - val_loss: 1.0630 - val_acc: 0.4333\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0095 - acc: 0.5002 - val_loss: 1.0627 - val_acc: 0.3500\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0470 - acc: 0.4301 - val_loss: 1.0611 - val_acc: 0.4167\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0090 - acc: 0.4999 - val_loss: 1.0286 - val_acc: 0.3833\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.9994 - acc: 0.4857 - val_loss: 1.0307 - val_acc: 0.4167\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0062 - acc: 0.4523 - val_loss: 1.0307 - val_acc: 0.5000\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0148 - acc: 0.4886 - val_loss: 1.0306 - val_acc: 0.4833\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.0068 - acc: 0.5014 - val_loss: 1.0151 - val_acc: 0.5167\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.0039 - acc: 0.4552 - val_loss: 1.0225 - val_acc: 0.4333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9957 - acc: 0.4958 - val_loss: 1.0148 - val_acc: 0.4167\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.9564 - acc: 0.5437 - val_loss: 0.9881 - val_acc: 0.5000\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.9535 - acc: 0.5200 - val_loss: 0.9735 - val_acc: 0.4667\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9530 - acc: 0.5266 - val_loss: 0.9772 - val_acc: 0.5000\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9487 - acc: 0.5302 - val_loss: 0.9720 - val_acc: 0.5333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9946 - acc: 0.4872 - val_loss: 0.9713 - val_acc: 0.4667\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9379 - acc: 0.5621 - val_loss: 0.9670 - val_acc: 0.4833\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9161 - acc: 0.5738 - val_loss: 0.9842 - val_acc: 0.4333\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.9675 - acc: 0.5285 - val_loss: 0.9574 - val_acc: 0.5167\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8982 - acc: 0.5889 - val_loss: 1.0505 - val_acc: 0.4000\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9046 - acc: 0.5622 - val_loss: 0.9576 - val_acc: 0.5000\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8838 - acc: 0.5847 - val_loss: 1.0122 - val_acc: 0.4167\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8995 - acc: 0.5723 - val_loss: 0.9559 - val_acc: 0.5167\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 24ms/step - loss: 1.1832 - acc: 0.3262 - val_loss: 1.1039 - val_acc: 0.3667\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.1252 - acc: 0.3555 - val_loss: 1.0981 - val_acc: 0.3667\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 14ms/step - loss: 1.1148 - acc: 0.3988 - val_loss: 1.0928 - val_acc: 0.4000\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.1280 - acc: 0.3394 - val_loss: 1.0956 - val_acc: 0.3667\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0890 - acc: 0.3808 - val_loss: 1.1024 - val_acc: 0.3333\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.1299 - acc: 0.3133 - val_loss: 1.0962 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0899 - acc: 0.3909 - val_loss: 1.0838 - val_acc: 0.3333\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0808 - acc: 0.3973 - val_loss: 1.0789 - val_acc: 0.4333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0741 - acc: 0.3537 - val_loss: 1.0715 - val_acc: 0.4667\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0758 - acc: 0.4023 - val_loss: 1.0612 - val_acc: 0.4667\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0746 - acc: 0.4172 - val_loss: 1.0632 - val_acc: 0.3833\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 1.0513 - acc: 0.4691 - val_loss: 1.0548 - val_acc: 0.4167\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0604 - acc: 0.4445 - val_loss: 1.0420 - val_acc: 0.4167\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0400 - acc: 0.4045 - val_loss: 1.0427 - val_acc: 0.4333\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 1.0142 - acc: 0.4686 - val_loss: 1.0247 - val_acc: 0.4333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0300 - acc: 0.4419 - val_loss: 1.0519 - val_acc: 0.4167\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0167 - acc: 0.4774 - val_loss: 1.0097 - val_acc: 0.4667\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0127 - acc: 0.4775 - val_loss: 1.0158 - val_acc: 0.5167\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9846 - acc: 0.4976 - val_loss: 1.0030 - val_acc: 0.4500\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0188 - acc: 0.4642 - val_loss: 0.9908 - val_acc: 0.5000\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9768 - acc: 0.5205 - val_loss: 0.9829 - val_acc: 0.5000\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9827 - acc: 0.5191 - val_loss: 0.9600 - val_acc: 0.5000\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9289 - acc: 0.4977 - val_loss: 0.9719 - val_acc: 0.4667\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9568 - acc: 0.5033 - val_loss: 0.9564 - val_acc: 0.4500\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.8946 - acc: 0.5558 - val_loss: 0.9418 - val_acc: 0.5000\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9783 - acc: 0.4859 - val_loss: 0.9552 - val_acc: 0.5333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9399 - acc: 0.5187 - val_loss: 0.9413 - val_acc: 0.5500\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9121 - acc: 0.5921 - val_loss: 0.9858 - val_acc: 0.4167\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9298 - acc: 0.5148 - val_loss: 0.9505 - val_acc: 0.5000\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9074 - acc: 0.5481 - val_loss: 0.9667 - val_acc: 0.4500\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.2352 - acc: 0.2843 - val_loss: 1.1071 - val_acc: 0.3000\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.1650 - acc: 0.3279 - val_loss: 1.0987 - val_acc: 0.2667\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.1143 - acc: 0.3555 - val_loss: 1.0856 - val_acc: 0.3333\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.1300 - acc: 0.3306 - val_loss: 1.0890 - val_acc: 0.4333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.1164 - acc: 0.3628 - val_loss: 1.0763 - val_acc: 0.4833\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0845 - acc: 0.3927 - val_loss: 1.0847 - val_acc: 0.4000\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0928 - acc: 0.3957 - val_loss: 1.0837 - val_acc: 0.3833\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.1014 - acc: 0.3579 - val_loss: 1.0791 - val_acc: 0.4167\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 1.0769 - acc: 0.4015 - val_loss: 1.0820 - val_acc: 0.4333\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0621 - acc: 0.3914 - val_loss: 1.0661 - val_acc: 0.4167\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0459 - acc: 0.4289 - val_loss: 1.0575 - val_acc: 0.4667\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0636 - acc: 0.4125 - val_loss: 1.0558 - val_acc: 0.4333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0639 - acc: 0.4366 - val_loss: 1.0542 - val_acc: 0.4833\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 1.0221 - acc: 0.4700 - val_loss: 1.0533 - val_acc: 0.4000\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0510 - acc: 0.4411 - val_loss: 1.0242 - val_acc: 0.4833\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0031 - acc: 0.4797 - val_loss: 1.0366 - val_acc: 0.4167\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9705 - acc: 0.5030 - val_loss: 1.0319 - val_acc: 0.5167\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0116 - acc: 0.4929 - val_loss: 1.0239 - val_acc: 0.4167\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.9520 - acc: 0.5338 - val_loss: 1.0064 - val_acc: 0.5000\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.9706 - acc: 0.5210 - val_loss: 0.9809 - val_acc: 0.5000\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.9521 - acc: 0.5297 - val_loss: 1.0098 - val_acc: 0.4167\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9636 - acc: 0.5312 - val_loss: 1.0071 - val_acc: 0.4000\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.9589 - acc: 0.5016 - val_loss: 0.9994 - val_acc: 0.4500\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.9123 - acc: 0.5567 - val_loss: 0.9782 - val_acc: 0.4500\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9556 - acc: 0.5069 - val_loss: 0.9946 - val_acc: 0.4667\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9658 - acc: 0.5187 - val_loss: 0.9765 - val_acc: 0.4833\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9135 - acc: 0.5185 - val_loss: 0.9629 - val_acc: 0.5167\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9368 - acc: 0.5090 - val_loss: 0.9660 - val_acc: 0.4833\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.9516 - acc: 0.5675 - val_loss: 0.9651 - val_acc: 0.5500\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9077 - acc: 0.5535 - val_loss: 0.9721 - val_acc: 0.4333\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 44ms/step - loss: 1.1820 - acc: 0.3094 - val_loss: 1.0838 - val_acc: 0.4167\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.1214 - acc: 0.3892 - val_loss: 1.0936 - val_acc: 0.3667\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.1027 - acc: 0.4489 - val_loss: 1.0901 - val_acc: 0.3833\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0969 - acc: 0.3795 - val_loss: 1.0768 - val_acc: 0.4167\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0741 - acc: 0.4056 - val_loss: 1.0745 - val_acc: 0.4333\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 1.0716 - acc: 0.4395 - val_loss: 1.0725 - val_acc: 0.4333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0700 - acc: 0.4644 - val_loss: 1.0548 - val_acc: 0.5167\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0390 - acc: 0.4395 - val_loss: 1.0498 - val_acc: 0.4667\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0416 - acc: 0.4779 - val_loss: 1.0303 - val_acc: 0.4500\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 1.0402 - acc: 0.4595 - val_loss: 1.0290 - val_acc: 0.4500\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9986 - acc: 0.4559 - val_loss: 1.0247 - val_acc: 0.4667\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9835 - acc: 0.5083 - val_loss: 1.0200 - val_acc: 0.4667\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9821 - acc: 0.5325 - val_loss: 0.9962 - val_acc: 0.4167\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9758 - acc: 0.4991 - val_loss: 0.9763 - val_acc: 0.4667\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9651 - acc: 0.5288 - val_loss: 0.9855 - val_acc: 0.4333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0070 - acc: 0.5004 - val_loss: 0.9729 - val_acc: 0.4667\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9180 - acc: 0.5646 - val_loss: 0.9888 - val_acc: 0.4667\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9436 - acc: 0.5410 - val_loss: 0.9542 - val_acc: 0.4167\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8948 - acc: 0.5638 - val_loss: 0.9653 - val_acc: 0.4833\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9295 - acc: 0.5570 - val_loss: 0.9593 - val_acc: 0.5333\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9111 - acc: 0.5277 - val_loss: 0.9869 - val_acc: 0.4667\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8581 - acc: 0.6128 - val_loss: 0.9385 - val_acc: 0.5000\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8753 - acc: 0.5913 - val_loss: 0.9320 - val_acc: 0.5167\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8535 - acc: 0.6109 - val_loss: 0.9332 - val_acc: 0.5500\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8069 - acc: 0.6167 - val_loss: 0.8911 - val_acc: 0.5667\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8364 - acc: 0.5892 - val_loss: 0.9081 - val_acc: 0.4667\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8245 - acc: 0.6041 - val_loss: 0.9044 - val_acc: 0.5167\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8111 - acc: 0.6273 - val_loss: 0.8981 - val_acc: 0.5333\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8541 - acc: 0.6085 - val_loss: 0.9041 - val_acc: 0.5333\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8261 - acc: 0.6111 - val_loss: 0.9183 - val_acc: 0.5333\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "bac_per_fold = []\n",
    "f1_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "for train, test in KFold(n_splits=5, shuffle=True).split(inputs, targets):\n",
    "    model_ex = models.Sequential()\n",
    "    model_ex.add(layers.Flatten())\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 1024))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 1024))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 1024))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 1024))\n",
    "    model_ex.add(layers.Dense(64, activation='relu', input_dim=6* 6* 1024))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block5_conv1':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model_ex.compile(optimizer=optimizers.SGD(lr= 0.0030),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    with device('/GPU:0'):\n",
    "        history = model_ex.fit(train_features, train_labels, epochs=30, batch_size=batch_size, validation_data=(validation_features, validation_labels))\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    predctions = np.argmax(model_ex.predict(inputs[test], batch_size=batch_size,verbose=0), axis=-1)\n",
    "    scores = model_ex.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    acc_per_fold.append((scores[1] * 100))\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    rounded_labels=np.argmax(targets[test], axis=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(rounded_labels, predctions, labels=[0,1]).ravel()\n",
    "    bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp)))\n",
    "    f1 = (2*tp)/(2*tp+fp+fn)\n",
    "    bac_per_fold.append(bac) # balanced accuracy\n",
    "    f1_per_fold.append(f1) # F1\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.08020468354225159 - Accuracy: 0.656000018119812%\n",
      ">       BAC: 0.6230769230769231 - F1: 0.47058823529411764%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.08457634449005128 - Accuracy: 0.656000018119812%\n",
      ">       BAC: 0.6871794871794872 - F1: 0.5555555555555556%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.0892037570476532 - Accuracy: 0.6399999856948853%\n",
      ">       BAC: 0.7336265884652982 - F1: 0.7213114754098361%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.08347463607788086 - Accuracy: 0.6480000019073486%\n",
      ">       BAC: 0.6366935483870968 - F1: 0.576271186440678%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.07663876414299012 - Accuracy: 0.6854838728904724%\n",
      ">       BAC: 0.7674516400336417 - F1: 0.7384615384615385%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.657096779346466 (+- 1.5383657323239621)\n",
      "> BAC: 0.6896056374284895 (+- 0.05518965384347649)\n",
      "> F1: 0.6124375982323451 (+- 0.1023722350342125)\n",
      "> Loss: 0.0828196370601654\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]/10} - Accuracy: {acc_per_fold[i]/100}%')\n",
    "    print(f'>       BAC: {bac_per_fold[i]} - F1: {f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)/100} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> BAC: {np.mean(bac_per_fold)} (+- {np.std(bac_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)/10}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "mylist = []\n",
    "for j in test_labels: mylist.append([int(i) for i in j])\n",
    "mylist = np.array(mylist)\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs_ex = model_ex.predict(test_features)\n",
    "preds_ex = probs_ex[:, 1]\n",
    "n_classes = 3\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(mylist[:, i], probs_ex[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "roc_aucAfter = []\n",
    "for i in range(3): roc_aucAfter.append(roc_auc[i])\n",
    "fprDenseNet121 = []\n",
    "tprDenseNet121 = []\n",
    "roc_aucDenseNet121 = []\n",
    "fprResNet50 = []\n",
    "tprResNet50 = []\n",
    "roc_aucResNet50 = []\n",
    "for i in range(3):\n",
    "    if roc_aucAfter[i] != max(roc_aucAfter) and roc_aucAfter[i] != min(roc_aucAfter):\n",
    "        fprDenseNet121 = fpr[i]\n",
    "        tprDenseNet121 = tpr[i]\n",
    "        roc_aucDenseNet121 = roc_auc[i]\n",
    "    if roc_aucAfter[i] == max(roc_aucAfter):\n",
    "        fprResNet50 = fpr[i]\n",
    "        tprResNet50 = tpr[i]\n",
    "        roc_aucResNet50 = roc_auc[i]\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(mylist.ravel(), probs_ex.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 210, 210, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 216, 216, 3)  0           input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 105, 105, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 105, 105, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 105, 105, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 107, 107, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 53, 53, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 53, 53, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 53, 53, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 53, 53, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 53, 53, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 53, 53, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 53, 53, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 53, 53, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 53, 53, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 53, 53, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 53, 53, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 53, 53, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 53, 53, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 53, 53, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 53, 53, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 53, 53, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 53, 53, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 53, 53, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 53, 53, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 53, 53, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 53, 53, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 53, 53, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 53, 53, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 53, 53, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 53, 53, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 53, 53, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 53, 53, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 27, 27, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 27, 27, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 27, 27, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 27, 27, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 27, 27, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 27, 27, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 27, 27, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 27, 27, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 27, 27, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 27, 27, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 27, 27, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 27, 27, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 27, 27, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 27, 27, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 27, 27, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 27, 27, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 27, 27, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 27, 27, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 27, 27, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 27, 27, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 27, 27, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 27, 27, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 27, 27, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 27, 27, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 27, 27, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 27, 27, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 27, 27, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 27, 27, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 27, 27, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 27, 27, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 27, 27, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 27, 27, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 27, 27, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 27, 27, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "conv_base = ResNet50(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(210,210, 3))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\feras\\\\Bovine data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 21\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "Found 84 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 2048))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    \n",
    "    if sample_count == 540:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1/255,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.2)\n",
    "    else:\n",
    "        train_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = train_datagen.flow_from_directory(directory,\n",
    "        target_size=(210,210),\n",
    "        batch_size = batch_size, \n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "train_features, train_labels = extract_features(train_dir, 540)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 60)\n",
    "test_features, test_labels = extract_features(test_dir, 84)\n",
    "##----------------------------------------------------------\n",
    "train_features = np.reshape(train_features, (540, 7* 7* 2048))\n",
    "validation_features = np.reshape(validation_features, (60, 7* 7* 2048))\n",
    "test_features = np.reshape(test_features, (84, 7* 7* 2048))\n",
    "inputs = np.concatenate((train_features, test_features), axis=0)\n",
    "targets = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 1.5173 - acc: 0.3464 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 2/15\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 1.0987 - acc: 0.3428 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 3/15\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 1.0987 - acc: 0.2997 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 4/15\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 1.0986 - acc: 0.3091 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 5/15\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 1.0988 - acc: 0.2838 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 6/15\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 1.0986 - acc: 0.3005 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 7/15\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 1.0987 - acc: 0.3192 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 8/15\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 1.0986 - acc: 0.2936 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 9/15\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 1.0986 - acc: 0.3157 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 10/15\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 1.0985 - acc: 0.3853 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 11/15\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 1.0986 - acc: 0.3457 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 12/15\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 1.0987 - acc: 0.2938 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 13/15\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 1.0987 - acc: 0.3474 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 14/15\n",
      "26/26 [==============================] - 1s 30ms/step - loss: 1.0987 - acc: 0.3486 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 15/15\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 1.0985 - acc: 0.3662 - val_loss: 1.0986 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-176-ce2a1faedfeb>:46: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp + 0.1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "26/26 [==============================] - 2s 38ms/step - loss: 1.6596 - acc: 0.3004 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 2/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0986 - acc: 0.3497 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 3/15\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0987 - acc: 0.3056 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 4/15\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0987 - acc: 0.3490 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 5/15\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 1.0986 - acc: 0.3279 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 6/15\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0986 - acc: 0.3337 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 7/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0987 - acc: 0.3146 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 8/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0986 - acc: 0.3129 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 9/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0987 - acc: 0.3010 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 10/15\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0986 - acc: 0.3036 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 11/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0986 - acc: 0.3292 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 12/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0987 - acc: 0.3495 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 13/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0987 - acc: 0.2987 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 14/15\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0986 - acc: 0.3700 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 15/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0985 - acc: 0.3457 - val_loss: 1.0986 - val_acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "bac_per_fold = []\n",
    "f1_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "for train, test in KFold(n_splits=2, shuffle=True).split(inputs, targets):\n",
    "    model_ex = models.Sequential()\n",
    "    model_ex.add(layers.Flatten())\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=7* 7* 2048))\n",
    "#     model_ex.add(layers.Dropout(0.5))\n",
    "#     model_ex.add(layers.Dense(128, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(64, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block5_conv1':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model_ex.compile(optimizer=optimizers.SGD(lr= 0.0030),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    with device('/GPU:0'):\n",
    "        history = model_ex.fit(train_features, train_labels, epochs=15, batch_size=batch_size, validation_data=(validation_features, validation_labels))\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    predctions = np.argmax(model_ex.predict(inputs[test], batch_size=batch_size,verbose=0), axis=-1)\n",
    "    scores = model_ex.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    acc_per_fold.append((scores[1] * 100))\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    rounded_labels=np.argmax(targets[test], axis=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(rounded_labels, predctions, labels=[0,1]).ravel()\n",
    "    if tp+fn == 0 or tn+fp == 0:\n",
    "        bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp + 0.1)))\n",
    "        f1 = (2*tp)/(2*tp+fp+fn + 0.1)\n",
    "    else:\n",
    "        bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp)))\n",
    "        f1 = (2*tp)/(2*tp+fp+fn)\n",
    "    bac_per_fold.append(bac) # balanced accuracy\n",
    "    f1_per_fold.append(f1) # F1\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.10985383987426758 - Accuracy: 0.375%\n",
      ">       BAC: nan - F1: 0.0%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.10985620021820068 - Accuracy: 0.3621794879436493%\n",
      ">       BAC: 0.5 - F1: 0.0%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.36858974397182465 (+- 0.6410256028175354)\n",
      "> BAC: nan (+- nan)\n",
      "> F1: 0.0 (+- 0.0)\n",
      "> Loss: 0.10985502004623413\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]/10} - Accuracy: {acc_per_fold[i]/100}%')\n",
    "    print(f'>       BAC: {bac_per_fold[i]} - F1: {f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)/100} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> BAC: {np.mean(bac_per_fold)} (+- {np.std(bac_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)/10}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.metrics as metrics\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# mylist = []\n",
    "# for j in test_labels: mylist.append([int(i) for i in j])\n",
    "# mylist = np.array(mylist)\n",
    "\n",
    "# # calculate the fpr and tpr for all thresholds of the classification\n",
    "# probs_ex = model_ex.predict(test_features)\n",
    "# preds_ex = probs_ex[:, 1]\n",
    "# n_classes = 3\n",
    "    \n",
    "# # Compute ROC curve and ROC area for each class\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# for i in range(n_classes):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(mylist[:, i], probs_ex[:, i])\n",
    "#     roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "# roc_aucAfter = []\n",
    "# for i in range(3): roc_aucAfter.append(roc_auc[i])\n",
    "# fprResNet50 = []\n",
    "# tprResNet50 = []\n",
    "# roc_aucResNet50 = []\n",
    "# for i in range(3):\n",
    "#     if roc_aucAfter[i] != max(roc_aucAfter) and roc_aucAfter[i] != min(roc_aucAfter):\n",
    "#         fprResNet50 = fpr[i]\n",
    "#         tprResNet50 = tpr[i]\n",
    "#         roc_aucResNet50 = roc_auc[i]\n",
    "#         o = 1\n",
    "# if roc_aucResNet50 == []:\n",
    "#     fprResNet50 = fpr[0]\n",
    "#     tprResNet50 = tpr[0]\n",
    "#     roc_aucResNet50 = roc_auc[0]\n",
    "        \n",
    "# # Compute micro-average ROC curve and ROC area\n",
    "# fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(mylist.ravel(), probs_ex.ravel())\n",
    "# roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fprVgg16, tprVgg16, color='darkorange',\n",
    "         lw=lw, label='Vgg16 (area = %0.2f)' % roc_aucVgg16)\n",
    "plt.plot(fprVgg19, tprVgg19, color='red',\n",
    "         lw=lw, label='Vgg19 (area = %0.2f)' % roc_aucVgg19)\n",
    "plt.plot(fprXception, tprXception, color='blue',\n",
    "         lw=lw, label='Xception (area = %0.2f)' % roc_aucXception)\n",
    "plt.plot(fprDenseNet121, tprDenseNet121, color='green',\n",
    "         lw=lw, label='DenseNet121 (area = %0.2f)' % roc_aucDenseNet121)\n",
    "plt.plot(fprResNet50, tprResNet50, color='gray',\n",
    "         lw=lw, label='ResNet50 (area = %0.2f)' % roc_aucResNet50)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic(Roc curve)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
