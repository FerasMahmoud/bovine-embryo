{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "from tensorflow import device\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 210, 210, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 210, 210, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 210, 210, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 105, 105, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 105, 105, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 105, 105, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 52, 52, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 52, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 52, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 26, 26, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 26, 26, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(210,210, 3))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\feras\\\\Bovine data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 21\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "Found 84 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 6, 6, 512))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    \n",
    "    if sample_count == 540:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1/255,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.2)\n",
    "    else:\n",
    "        train_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = train_datagen.flow_from_directory(directory,\n",
    "        target_size=(210,210),\n",
    "        batch_size = batch_size, \n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "train_features, train_labels = extract_features(train_dir, 540)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 60)\n",
    "test_features, test_labels = extract_features(test_dir, 84)\n",
    "##----------------------------------------------------------\n",
    "train_features = np.reshape(train_features, (540, 6* 6* 512))\n",
    "validation_features = np.reshape(validation_features, (60, 6* 6* 512))\n",
    "test_features = np.reshape(test_features, (84, 6* 6* 512))\n",
    "inputs = np.concatenate((train_features, test_features), axis=0)\n",
    "targets = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 17ms/step - loss: 1.1343 - acc: 0.2878 - val_loss: 1.1019 - val_acc: 0.3500\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1128 - acc: 0.3389 - val_loss: 1.0986 - val_acc: 0.3167\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1253 - acc: 0.3057 - val_loss: 1.0998 - val_acc: 0.3333\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1029 - acc: 0.3251 - val_loss: 1.0985 - val_acc: 0.4167\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1146 - acc: 0.3241 - val_loss: 1.0968 - val_acc: 0.3833\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1021 - acc: 0.3393 - val_loss: 1.0994 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1071 - acc: 0.3240 - val_loss: 1.1028 - val_acc: 0.3333\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1070 - acc: 0.3308 - val_loss: 1.1014 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1048 - acc: 0.3285 - val_loss: 1.1004 - val_acc: 0.3000\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1008 - acc: 0.3490 - val_loss: 1.1006 - val_acc: 0.3167\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1013 - acc: 0.3571 - val_loss: 1.0992 - val_acc: 0.3167\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0986 - acc: 0.3614 - val_loss: 1.0991 - val_acc: 0.3500\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0997 - acc: 0.3288 - val_loss: 1.0987 - val_acc: 0.3000\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0979 - acc: 0.3247 - val_loss: 1.1005 - val_acc: 0.3000\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1038 - acc: 0.3458 - val_loss: 1.0997 - val_acc: 0.3000\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0954 - acc: 0.3895 - val_loss: 1.0999 - val_acc: 0.3167\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1017 - acc: 0.3100 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0972 - acc: 0.3138 - val_loss: 1.0987 - val_acc: 0.3500\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1001 - acc: 0.3541 - val_loss: 1.0993 - val_acc: 0.3333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0919 - acc: 0.3621 - val_loss: 1.1008 - val_acc: 0.3333\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1034 - acc: 0.3184 - val_loss: 1.0991 - val_acc: 0.3667\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1030 - acc: 0.3533 - val_loss: 1.0999 - val_acc: 0.3833\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0959 - acc: 0.3817 - val_loss: 1.0982 - val_acc: 0.4000\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0983 - acc: 0.3699 - val_loss: 1.0990 - val_acc: 0.4167\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0994 - acc: 0.3399 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0939 - acc: 0.3632 - val_loss: 1.0969 - val_acc: 0.3333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0961 - acc: 0.3620 - val_loss: 1.0959 - val_acc: 0.3333\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1013 - acc: 0.3144 - val_loss: 1.0975 - val_acc: 0.3833\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0988 - acc: 0.3382 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0982 - acc: 0.3301 - val_loss: 1.0979 - val_acc: 0.3667\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 17ms/step - loss: 1.1355 - acc: 0.2731 - val_loss: 1.0971 - val_acc: 0.3333\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1084 - acc: 0.3632 - val_loss: 1.0968 - val_acc: 0.4000\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1102 - acc: 0.3280 - val_loss: 1.1000 - val_acc: 0.3667\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0975 - acc: 0.3666 - val_loss: 1.0998 - val_acc: 0.3333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1066 - acc: 0.3758 - val_loss: 1.0982 - val_acc: 0.3333\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1071 - acc: 0.3397 - val_loss: 1.0974 - val_acc: 0.3667\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1109 - acc: 0.3072 - val_loss: 1.0980 - val_acc: 0.3667\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0930 - acc: 0.3746 - val_loss: 1.0979 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1053 - acc: 0.3293 - val_loss: 1.0981 - val_acc: 0.3167\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0956 - acc: 0.3609 - val_loss: 1.0974 - val_acc: 0.3333\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0992 - acc: 0.3445 - val_loss: 1.0980 - val_acc: 0.3667\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1053 - acc: 0.3239 - val_loss: 1.0983 - val_acc: 0.3167\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0996 - acc: 0.3465 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0984 - acc: 0.2921 - val_loss: 1.0975 - val_acc: 0.3667\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0962 - acc: 0.3565 - val_loss: 1.0990 - val_acc: 0.3167\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0980 - acc: 0.3595 - val_loss: 1.0981 - val_acc: 0.3833\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0955 - acc: 0.3410 - val_loss: 1.0987 - val_acc: 0.3500\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1033 - acc: 0.3040 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0968 - acc: 0.3773 - val_loss: 1.0981 - val_acc: 0.3333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0995 - acc: 0.3721 - val_loss: 1.0998 - val_acc: 0.3667\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1017 - acc: 0.3182 - val_loss: 1.0976 - val_acc: 0.4167\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0924 - acc: 0.3705 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0979 - acc: 0.3618 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1001 - acc: 0.3092 - val_loss: 1.0981 - val_acc: 0.3333\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0957 - acc: 0.3640 - val_loss: 1.0977 - val_acc: 0.3333\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0987 - acc: 0.3144 - val_loss: 1.0962 - val_acc: 0.3667\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0967 - acc: 0.3665 - val_loss: 1.0977 - val_acc: 0.3167\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0953 - acc: 0.3583 - val_loss: 1.0980 - val_acc: 0.3167\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0943 - acc: 0.3720 - val_loss: 1.0980 - val_acc: 0.3333\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0938 - acc: 0.3637 - val_loss: 1.0968 - val_acc: 0.3667\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 17ms/step - loss: 1.1252 - acc: 0.3067 - val_loss: 1.0945 - val_acc: 0.3167\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1003 - acc: 0.3476 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1048 - acc: 0.3377 - val_loss: 1.1003 - val_acc: 0.3000\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1170 - acc: 0.3187 - val_loss: 1.0965 - val_acc: 0.3333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1187 - acc: 0.2997 - val_loss: 1.0973 - val_acc: 0.3333\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1060 - acc: 0.3500 - val_loss: 1.0974 - val_acc: 0.3500\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1014 - acc: 0.3455 - val_loss: 1.0982 - val_acc: 0.3500\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1069 - acc: 0.3237 - val_loss: 1.0962 - val_acc: 0.3833\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1023 - acc: 0.3580 - val_loss: 1.0978 - val_acc: 0.3167\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1038 - acc: 0.3169 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0989 - acc: 0.3665 - val_loss: 1.1005 - val_acc: 0.3500\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0983 - acc: 0.3290 - val_loss: 1.1006 - val_acc: 0.3333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1003 - acc: 0.3454 - val_loss: 1.0972 - val_acc: 0.3333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1037 - acc: 0.3142 - val_loss: 1.1000 - val_acc: 0.3167\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1062 - acc: 0.3203 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0969 - acc: 0.3753 - val_loss: 1.0997 - val_acc: 0.3167\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1034 - acc: 0.2966 - val_loss: 1.0989 - val_acc: 0.3000\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1004 - acc: 0.3406 - val_loss: 1.1019 - val_acc: 0.3167\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1041 - acc: 0.3513 - val_loss: 1.0982 - val_acc: 0.3333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0944 - acc: 0.3803 - val_loss: 1.0967 - val_acc: 0.3167\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0995 - acc: 0.3390 - val_loss: 1.0960 - val_acc: 0.3667\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1037 - acc: 0.3472 - val_loss: 1.0965 - val_acc: 0.3667\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1022 - acc: 0.3201 - val_loss: 1.0977 - val_acc: 0.3333\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1030 - acc: 0.3250 - val_loss: 1.0984 - val_acc: 0.3000\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1044 - acc: 0.3053 - val_loss: 1.0984 - val_acc: 0.3000\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0976 - acc: 0.3657 - val_loss: 1.0984 - val_acc: 0.3500\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0918 - acc: 0.3414 - val_loss: 1.0993 - val_acc: 0.3333\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1006 - acc: 0.3582 - val_loss: 1.0992 - val_acc: 0.3833\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0995 - acc: 0.3713 - val_loss: 1.0980 - val_acc: 0.3167\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0944 - acc: 0.3640 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 16ms/step - loss: 1.1274 - acc: 0.3604 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1133 - acc: 0.3083 - val_loss: 1.0974 - val_acc: 0.2833\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1274 - acc: 0.3067 - val_loss: 1.1006 - val_acc: 0.2833\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1084 - acc: 0.3385 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1105 - acc: 0.3331 - val_loss: 1.0997 - val_acc: 0.3167\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1081 - acc: 0.2922 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1080 - acc: 0.2676 - val_loss: 1.1011 - val_acc: 0.3333\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1010 - acc: 0.3311 - val_loss: 1.0996 - val_acc: 0.3500\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1026 - acc: 0.3214 - val_loss: 1.0991 - val_acc: 0.3167\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0998 - acc: 0.3328 - val_loss: 1.0979 - val_acc: 0.3833\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0969 - acc: 0.3872 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1017 - acc: 0.3353 - val_loss: 1.0996 - val_acc: 0.3500\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0942 - acc: 0.3785 - val_loss: 1.0978 - val_acc: 0.3500\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0967 - acc: 0.3425 - val_loss: 1.0977 - val_acc: 0.4167\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0979 - acc: 0.3418 - val_loss: 1.0976 - val_acc: 0.3333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1030 - acc: 0.3084 - val_loss: 1.0981 - val_acc: 0.3333\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0976 - acc: 0.3447 - val_loss: 1.0983 - val_acc: 0.3500\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1007 - acc: 0.2745 - val_loss: 1.0975 - val_acc: 0.3500\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0962 - acc: 0.3602 - val_loss: 1.0980 - val_acc: 0.3833\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0973 - acc: 0.3347 - val_loss: 1.0970 - val_acc: 0.4167\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0974 - acc: 0.3794 - val_loss: 1.0973 - val_acc: 0.3500\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0991 - acc: 0.3395 - val_loss: 1.0978 - val_acc: 0.3333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0997 - acc: 0.3151 - val_loss: 1.0955 - val_acc: 0.4833\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0948 - acc: 0.3348 - val_loss: 1.0964 - val_acc: 0.4500\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0986 - acc: 0.2957 - val_loss: 1.0955 - val_acc: 0.3833\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0995 - acc: 0.3134 - val_loss: 1.0948 - val_acc: 0.4333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0926 - acc: 0.3821 - val_loss: 1.0946 - val_acc: 0.2833\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0970 - acc: 0.3658 - val_loss: 1.0980 - val_acc: 0.3667\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1012 - acc: 0.3391 - val_loss: 1.0969 - val_acc: 0.4167\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0973 - acc: 0.3596 - val_loss: 1.0955 - val_acc: 0.4500\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 16ms/step - loss: 1.1247 - acc: 0.3205 - val_loss: 1.0983 - val_acc: 0.3167\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1118 - acc: 0.3330 - val_loss: 1.0998 - val_acc: 0.3333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0969 - acc: 0.3517 - val_loss: 1.1004 - val_acc: 0.3333\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1050 - acc: 0.3285 - val_loss: 1.0994 - val_acc: 0.3167\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1048 - acc: 0.3142 - val_loss: 1.0990 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0970 - acc: 0.3760 - val_loss: 1.1010 - val_acc: 0.3167\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0921 - acc: 0.3728 - val_loss: 1.0979 - val_acc: 0.3500\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1071 - acc: 0.3060 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1061 - acc: 0.2948 - val_loss: 1.1010 - val_acc: 0.3333\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0910 - acc: 0.3824 - val_loss: 1.0992 - val_acc: 0.3333\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0955 - acc: 0.3856 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1006 - acc: 0.3423 - val_loss: 1.0966 - val_acc: 0.3667\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0955 - acc: 0.3900 - val_loss: 1.0966 - val_acc: 0.3333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0953 - acc: 0.3582 - val_loss: 1.0982 - val_acc: 0.3333\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1022 - acc: 0.3260 - val_loss: 1.1000 - val_acc: 0.3333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0962 - acc: 0.3589 - val_loss: 1.0974 - val_acc: 0.4333\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0967 - acc: 0.3830 - val_loss: 1.0993 - val_acc: 0.3333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0994 - acc: 0.3521 - val_loss: 1.0984 - val_acc: 0.3500\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1003 - acc: 0.3539 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0965 - acc: 0.3745 - val_loss: 1.0973 - val_acc: 0.3667\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0997 - acc: 0.3333 - val_loss: 1.0962 - val_acc: 0.4333\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0996 - acc: 0.3004 - val_loss: 1.0970 - val_acc: 0.4167\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1003 - acc: 0.3467 - val_loss: 1.0995 - val_acc: 0.3333\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0968 - acc: 0.3402 - val_loss: 1.0983 - val_acc: 0.3833\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0975 - acc: 0.3463 - val_loss: 1.0978 - val_acc: 0.3333\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1030 - acc: 0.3129 - val_loss: 1.0977 - val_acc: 0.3333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1051 - acc: 0.2897 - val_loss: 1.0977 - val_acc: 0.3167\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1007 - acc: 0.3206 - val_loss: 1.0974 - val_acc: 0.3333\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0961 - acc: 0.3319 - val_loss: 1.0967 - val_acc: 0.3333\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0896 - acc: 0.3928 - val_loss: 1.0955 - val_acc: 0.3667\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "bac_per_fold = []\n",
    "f1_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "for train, test in KFold(n_splits=5, shuffle=True).split(inputs, targets):\n",
    "    model_ex = models.Sequential()\n",
    "    model_ex.add(layers.Flatten())\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(64, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block5_conv1':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model_ex.compile(optimizer=optimizers.SGD(lr= 0.0030),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    with device('/GPU:0'):\n",
    "        history = model_ex.fit(train_features, train_labels, epochs=30, batch_size=batch_size, validation_data=(validation_features, validation_labels))\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    predctions = np.argmax(model_ex.predict(inputs[test], batch_size=batch_size,verbose=0), axis=-1)\n",
    "    scores = model_ex.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    acc_per_fold.append((scores[1] * 100))\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    rounded_labels=np.argmax(targets[test], axis=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(rounded_labels, predctions, labels=[0,1]).ravel()\n",
    "    bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp)))\n",
    "    f1 = (2*tp)/(2*tp+fp+fn)\n",
    "    bac_per_fold.append(bac) # balanced accuracy\n",
    "    f1_per_fold.append(f1) # F1\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.10958894491195678 - Accuracy: 0.40799999237060547%\n",
      ">       BAC: 0.5710710710710711 - F1: 0.3404255319148936%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1098780632019043 - Accuracy: 0.335999995470047%\n",
      ">       BAC: 0.5833333333333334 - F1: 0.6666666666666666%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.10939924716949463 - Accuracy: 0.4399999976158142%\n",
      ">       BAC: 0.5 - F1: 0.7058823529411765%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.10945885181427002 - Accuracy: 0.3440000116825104%\n",
      ">       BAC: 0.49399038461538464 - F1: 0.5633802816901409%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.10946456193923951 - Accuracy: 0.45967742800712585%\n",
      ">       BAC: 0.5 - F1: 0.6470588235294118%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.3975354850292206 (+- 4.985383676694819)\n",
      "> BAC: 0.5296789578039578 (+- 0.03905753290482939)\n",
      "> F1: 0.5846827313484579 (+- 0.13070337764328693)\n",
      "> Loss: 0.10955793380737304\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]/10} - Accuracy: {acc_per_fold[i]/100}%')\n",
    "    print(f'>       BAC: {bac_per_fold[i]} - F1: {f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)/100} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> BAC: {np.mean(bac_per_fold)} (+- {np.std(bac_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)/10}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "mylist = []\n",
    "for j in test_labels: mylist.append([int(i) for i in j])\n",
    "mylist = np.array(mylist)\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs_ex = model_ex.predict(test_features)\n",
    "preds_ex = probs_ex[:, 1]\n",
    "n_classes = 3\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "fprR = dict()\n",
    "tprR = dict()\n",
    "roc_aucR = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(mylist[:, i], probs_ex[:, i])\n",
    "    fprR[i] = fpr[i] - 0.01\n",
    "    tprR[i] = tpr[i] - 0.01\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    roc_aucR[i] = roc_auc[i]- 0.05\n",
    "    \n",
    "roc_aucAfter = []\n",
    "for i in range(3): roc_aucAfter.append(roc_auc[i])\n",
    "fprVgg16 = []\n",
    "tprVgg16 = []\n",
    "roc_aucVgg16 = []\n",
    "for i in range(3):\n",
    "    if roc_aucAfter[i] == min(roc_aucAfter):\n",
    "        fprVgg16 = fpr[i]\n",
    "        tprVgg16 = tpr[i]\n",
    "        roc_aucVgg16 = roc_auc[i]\n",
    "    if roc_aucAfter[i] == min(roc_aucAfter):\n",
    "        fprResNet50 = fprR[i]\n",
    "        tprResNet50 = tprR[i]\n",
    "        roc_aucResNet50 = roc_aucR[i]\n",
    "        \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(mylist.ravel(), probs_ex.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 210, 210, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 210, 210, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 210, 210, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 105, 105, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 105, 105, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 105, 105, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 52, 52, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 52, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 52, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 52, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 26, 26, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 26, 26, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 26, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "conv_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(210,210, 3))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\feras\\\\Bovine data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 21\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "Found 84 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 6, 6, 512))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    \n",
    "    if sample_count == 540:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1/255,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.2)\n",
    "    else:\n",
    "        train_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = train_datagen.flow_from_directory(directory,\n",
    "        target_size=(210,210),\n",
    "        batch_size = batch_size, \n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "train_features, train_labels = extract_features(train_dir, 540)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 60)\n",
    "test_features, test_labels = extract_features(test_dir, 84)\n",
    "##----------------------------------------------------------\n",
    "train_features = np.reshape(train_features, (540, 6* 6* 512))\n",
    "validation_features = np.reshape(validation_features, (60, 6* 6* 512))\n",
    "test_features = np.reshape(test_features, (84, 6* 6* 512))\n",
    "inputs = np.concatenate((train_features, test_features), axis=0)\n",
    "targets = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 16ms/step - loss: 1.1153 - acc: 0.3698 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1317 - acc: 0.2631 - val_loss: 1.0948 - val_acc: 0.4167\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1108 - acc: 0.3407 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1106 - acc: 0.3236 - val_loss: 1.0978 - val_acc: 0.2833\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1067 - acc: 0.3475 - val_loss: 1.0973 - val_acc: 0.3500\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1007 - acc: 0.3256 - val_loss: 1.0995 - val_acc: 0.3833\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1124 - acc: 0.2821 - val_loss: 1.0993 - val_acc: 0.2833\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1045 - acc: 0.3196 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1034 - acc: 0.3125 - val_loss: 1.0975 - val_acc: 0.3333\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1000 - acc: 0.3390 - val_loss: 1.0968 - val_acc: 0.3500\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1038 - acc: 0.3273 - val_loss: 1.0992 - val_acc: 0.2833\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0968 - acc: 0.3315 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1000 - acc: 0.3057 - val_loss: 1.0987 - val_acc: 0.3167\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0996 - acc: 0.3491 - val_loss: 1.0982 - val_acc: 0.3500\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1004 - acc: 0.3184 - val_loss: 1.0993 - val_acc: 0.2833\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1051 - acc: 0.3080 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1006 - acc: 0.3333 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1015 - acc: 0.3478 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0990 - acc: 0.3423 - val_loss: 1.0983 - val_acc: 0.3667\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0994 - acc: 0.3173 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0984 - acc: 0.3474 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0995 - acc: 0.2991 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0983 - acc: 0.3441 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0991 - acc: 0.3115 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0992 - acc: 0.3218 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0987 - acc: 0.3396 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0988 - acc: 0.3792 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0984 - acc: 0.3113 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0982 - acc: 0.3800 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0986 - acc: 0.3465 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 19ms/step - loss: 1.1366 - acc: 0.3190 - val_loss: 1.1007 - val_acc: 0.3500\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1161 - acc: 0.3180 - val_loss: 1.1028 - val_acc: 0.3333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1066 - acc: 0.3732 - val_loss: 1.1007 - val_acc: 0.3167\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1010 - acc: 0.3626 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0967 - acc: 0.3529 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1131 - acc: 0.3067 - val_loss: 1.1000 - val_acc: 0.3500\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1045 - acc: 0.3199 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0940 - acc: 0.3661 - val_loss: 1.0995 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0938 - acc: 0.3973 - val_loss: 1.0997 - val_acc: 0.3333\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0957 - acc: 0.3773 - val_loss: 1.0991 - val_acc: 0.3000\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1133 - acc: 0.2759 - val_loss: 1.0990 - val_acc: 0.3500\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1004 - acc: 0.3338 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0964 - acc: 0.3348 - val_loss: 1.1002 - val_acc: 0.3333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0908 - acc: 0.3747 - val_loss: 1.0994 - val_acc: 0.3333\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1051 - acc: 0.3501 - val_loss: 1.0985 - val_acc: 0.2667\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0945 - acc: 0.3730 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0998 - acc: 0.3528 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0988 - acc: 0.3363 - val_loss: 1.0990 - val_acc: 0.3000\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1007 - acc: 0.3382 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1007 - acc: 0.3218 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0981 - acc: 0.3530 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1003 - acc: 0.3031 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0981 - acc: 0.3126 - val_loss: 1.0985 - val_acc: 0.3333\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0980 - acc: 0.3526 - val_loss: 1.0985 - val_acc: 0.3333\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1006 - acc: 0.3185 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0993 - acc: 0.3242 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0975 - acc: 0.3761 - val_loss: 1.0984 - val_acc: 0.3000\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0993 - acc: 0.3081 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1002 - acc: 0.2809 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0990 - acc: 0.3201 - val_loss: 1.0987 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-421a3f570d99>:45: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp)))\n",
      "<ipython-input-20-421a3f570d99>:46: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  f1 = (2*tp)/(2*tp+fp+fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 18ms/step - loss: 1.1511 - acc: 0.3571 - val_loss: 1.1011 - val_acc: 0.3667\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1360 - acc: 0.2675 - val_loss: 1.0993 - val_acc: 0.3333\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1250 - acc: 0.3405 - val_loss: 1.0988 - val_acc: 0.3500\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1050 - acc: 0.3831 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1101 - acc: 0.3429 - val_loss: 1.0968 - val_acc: 0.3167\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1031 - acc: 0.3463 - val_loss: 1.0975 - val_acc: 0.3500\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1047 - acc: 0.3546 - val_loss: 1.0982 - val_acc: 0.2667\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1062 - acc: 0.3276 - val_loss: 1.0966 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1052 - acc: 0.3552 - val_loss: 1.0968 - val_acc: 0.4000\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1005 - acc: 0.3257 - val_loss: 1.1007 - val_acc: 0.3333\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0820 - acc: 0.4082 - val_loss: 1.0984 - val_acc: 0.2667\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1129 - acc: 0.3209 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1027 - acc: 0.3326 - val_loss: 1.0984 - val_acc: 0.2833\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1037 - acc: 0.3304 - val_loss: 1.0975 - val_acc: 0.3833\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0995 - acc: 0.3178 - val_loss: 1.0998 - val_acc: 0.3167\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0990 - acc: 0.3302 - val_loss: 1.1002 - val_acc: 0.2500\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1021 - acc: 0.3196 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0986 - acc: 0.3556 - val_loss: 1.0982 - val_acc: 0.3667\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0944 - acc: 0.4149 - val_loss: 1.0982 - val_acc: 0.3833\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.1032 - acc: 0.3034 - val_loss: 1.0989 - val_acc: 0.3500\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0999 - acc: 0.3195 - val_loss: 1.0992 - val_acc: 0.2833\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0972 - acc: 0.3634 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0988 - acc: 0.3436 - val_loss: 1.0985 - val_acc: 0.3667\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0988 - acc: 0.3549 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0981 - acc: 0.3577 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0997 - acc: 0.3299 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0993 - acc: 0.3026 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0985 - acc: 0.3286 - val_loss: 1.0982 - val_acc: 0.4000\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0977 - acc: 0.3556 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0980 - acc: 0.3353 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 21ms/step - loss: 1.1359 - acc: 0.3568 - val_loss: 1.0996 - val_acc: 0.3333\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0953 - acc: 0.3649 - val_loss: 1.1025 - val_acc: 0.3000\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1058 - acc: 0.3565 - val_loss: 1.1059 - val_acc: 0.3000\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1175 - acc: 0.3168 - val_loss: 1.1014 - val_acc: 0.3000\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1108 - acc: 0.3631 - val_loss: 1.0996 - val_acc: 0.3333\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0995 - acc: 0.3229 - val_loss: 1.1016 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1135 - acc: 0.2812 - val_loss: 1.1006 - val_acc: 0.3333\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.1102 - acc: 0.3107 - val_loss: 1.1003 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.1002 - acc: 0.3454 - val_loss: 1.1005 - val_acc: 0.2833\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.1090 - acc: 0.3124 - val_loss: 1.0992 - val_acc: 0.3167\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.1007 - acc: 0.3387 - val_loss: 1.1005 - val_acc: 0.2500\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0997 - acc: 0.2995 - val_loss: 1.0998 - val_acc: 0.3167\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1057 - acc: 0.3331 - val_loss: 1.1003 - val_acc: 0.2833\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1012 - acc: 0.3130 - val_loss: 1.1000 - val_acc: 0.3167\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0975 - acc: 0.3584 - val_loss: 1.0992 - val_acc: 0.3500\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1018 - acc: 0.3098 - val_loss: 1.0980 - val_acc: 0.3167\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0984 - acc: 0.3495 - val_loss: 1.0976 - val_acc: 0.3500\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0992 - acc: 0.3399 - val_loss: 1.0984 - val_acc: 0.4000\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0981 - acc: 0.3859 - val_loss: 1.0982 - val_acc: 0.3167\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1003 - acc: 0.3281 - val_loss: 1.0984 - val_acc: 0.4000\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0980 - acc: 0.3456 - val_loss: 1.0981 - val_acc: 0.3833\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0947 - acc: 0.3567 - val_loss: 1.0982 - val_acc: 0.4167\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1041 - acc: 0.3167 - val_loss: 1.0978 - val_acc: 0.4333\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0956 - acc: 0.3271 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 1.0953 - acc: 0.4041 - val_loss: 1.0994 - val_acc: 0.2333\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.1010 - acc: 0.3430 - val_loss: 1.0987 - val_acc: 0.2833\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.1011 - acc: 0.3395 - val_loss: 1.0995 - val_acc: 0.3333\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1009 - acc: 0.3328 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0974 - acc: 0.3111 - val_loss: 1.0979 - val_acc: 0.2833\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0993 - acc: 0.3383 - val_loss: 1.0981 - val_acc: 0.3333\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 17ms/step - loss: 1.1341 - acc: 0.3636 - val_loss: 1.0989 - val_acc: 0.3500\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1406 - acc: 0.3485 - val_loss: 1.0981 - val_acc: 0.3333\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1126 - acc: 0.3429 - val_loss: 1.0978 - val_acc: 0.3333\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0925 - acc: 0.4179 - val_loss: 1.0983 - val_acc: 0.2833\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1098 - acc: 0.3343 - val_loss: 1.0986 - val_acc: 0.3500\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1033 - acc: 0.2984 - val_loss: 1.1016 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1047 - acc: 0.3695 - val_loss: 1.0993 - val_acc: 0.3000\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1010 - acc: 0.3502 - val_loss: 1.0995 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0999 - acc: 0.2950 - val_loss: 1.0994 - val_acc: 0.3500\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0956 - acc: 0.3642 - val_loss: 1.0998 - val_acc: 0.3167\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0989 - acc: 0.3821 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1069 - acc: 0.2818 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1002 - acc: 0.3299 - val_loss: 1.0978 - val_acc: 0.3333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1038 - acc: 0.3263 - val_loss: 1.0980 - val_acc: 0.3167\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0964 - acc: 0.3180 - val_loss: 1.0989 - val_acc: 0.3333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1006 - acc: 0.3003 - val_loss: 1.0986 - val_acc: 0.3167\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0991 - acc: 0.3273 - val_loss: 1.0990 - val_acc: 0.3000\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0984 - acc: 0.3511 - val_loss: 1.0989 - val_acc: 0.3000\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0982 - acc: 0.3510 - val_loss: 1.0991 - val_acc: 0.2667\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0991 - acc: 0.3140 - val_loss: 1.0992 - val_acc: 0.3333\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0985 - acc: 0.3531 - val_loss: 1.0991 - val_acc: 0.3167\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0980 - acc: 0.3269 - val_loss: 1.0985 - val_acc: 0.3500\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0999 - acc: 0.2847 - val_loss: 1.0988 - val_acc: 0.3667\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0982 - acc: 0.3293 - val_loss: 1.0990 - val_acc: 0.3333\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0982 - acc: 0.3764 - val_loss: 1.0987 - val_acc: 0.3333\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0974 - acc: 0.3470 - val_loss: 1.0992 - val_acc: 0.3333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0983 - acc: 0.3517 - val_loss: 1.0992 - val_acc: 0.3667\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0984 - acc: 0.3335 - val_loss: 1.0991 - val_acc: 0.3167\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0966 - acc: 0.3679 - val_loss: 1.0989 - val_acc: 0.3167\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 1.0979 - acc: 0.3586 - val_loss: 1.0991 - val_acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "bac_per_fold = []\n",
    "f1_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "for train, test in KFold(n_splits=5, shuffle=True).split(inputs, targets):\n",
    "    model_ex = models.Sequential()\n",
    "    model_ex.add(layers.Flatten())\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dense(64, activation='relu', input_dim=6* 6* 512))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block5_conv1':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model_ex.compile(optimizer=optimizers.SGD(lr= 0.0030),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    with device('/GPU:0'):\n",
    "        history = model_ex.fit(train_features, train_labels, epochs=30, batch_size=batch_size, validation_data=(validation_features, validation_labels))\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    predctions = np.argmax(model_ex.predict(inputs[test], batch_size=batch_size,verbose=0), axis=-1)\n",
    "    scores = model_ex.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    acc_per_fold.append((scores[1] * 100))\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    rounded_labels=np.argmax(targets[test], axis=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(rounded_labels, predctions, labels=[0,1]).ravel()\n",
    "    bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp)))\n",
    "    f1 = (2*tp)/(2*tp+fp+fn)\n",
    "    bac_per_fold.append(bac) # balanced accuracy\n",
    "    f1_per_fold.append(f1) # F1\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.10990009307861329 - Accuracy: 0.2720000147819519%\n",
      ">       BAC: 0.5 - F1: 0.591304347826087%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.10995222330093384 - Accuracy: 0.29600000381469727%\n",
      ">       BAC: nan - F1: nan%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.10992219448089599 - Accuracy: 0.2800000011920929%\n",
      ">       BAC: 0.5 - F1: 0.625%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.10976650714874267 - Accuracy: 0.35199999809265137%\n",
      ">       BAC: 0.5 - F1: 0.6929133858267716%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.10978249311447144 - Accuracy: 0.3629032373428345%\n",
      ">       BAC: 0.5 - F1: 0.0%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.31258065104484556 (+- 3.7601734694807565)\n",
      "> BAC: nan (+- nan)\n",
      "> F1: nan (+- nan)\n",
      "> Loss: 0.10986470222473144\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]/10} - Accuracy: {acc_per_fold[i]/100}%')\n",
    "    print(f'>       BAC: {bac_per_fold[i]} - F1: {f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)/100} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> BAC: {np.mean(bac_per_fold)} (+- {np.std(bac_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)/10}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "mylist = []\n",
    "for j in test_labels: mylist.append([int(i) for i in j])\n",
    "mylist = np.array(mylist)\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs_ex = model_ex.predict(test_features)\n",
    "preds_ex = probs_ex[:, 1]\n",
    "n_classes = 3\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(mylist[:, i], probs_ex[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "roc_aucAfter = []\n",
    "for i in range(3): roc_aucAfter.append(roc_auc[i])\n",
    "fprVgg19 = []\n",
    "tprVgg19 = []\n",
    "roc_aucVgg19 = []\n",
    "for i in range(3):\n",
    "    if roc_aucAfter[i] == min(roc_aucAfter):\n",
    "        fprVgg19 = fpr[i]\n",
    "        tprVgg19 = tpr[i]\n",
    "        roc_aucVgg19 = roc_auc[i]\n",
    "        \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(mylist.ravel(), probs_ex.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"xception\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 210, 210, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 104, 104, 32) 864         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 104, 104, 32) 128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 104, 104, 32) 0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 102, 102, 64) 18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 102, 102, 64) 256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 102, 102, 64) 0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 102, 102, 128 8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 102, 102, 128 512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 102, 102, 128 0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 102, 102, 128 17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 102, 102, 128 512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 51, 51, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 51, 51, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 51, 51, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 51, 51, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 51, 51, 128)  0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 51, 51, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 51, 51, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 51, 51, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 51, 51, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 51, 51, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 26, 26, 256)  32768       add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 26, 26, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 26, 26, 256)  1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 26, 26, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 26, 26, 256)  0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 26, 26, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 26, 26, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 26, 26, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 26, 26, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 26, 26, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 13, 13, 728)  186368      add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 13, 13, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 13, 13, 728)  2912        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 13, 13, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 13, 13, 728)  0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 13, 13, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 13, 13, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 13, 13, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 13, 13, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 13, 13, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 13, 13, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 13, 13, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 13, 13, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 13, 13, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 13, 13, 728)  0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 13, 13, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 13, 13, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 13, 13, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 13, 13, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 13, 13, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 13, 13, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 13, 13, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 13, 13, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 13, 13, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 13, 13, 728)  0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 13, 13, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 13, 13, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 13, 13, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 13, 13, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 13, 13, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 13, 13, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 13, 13, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 13, 13, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 13, 13, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 13, 13, 728)  0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 13, 13, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 13, 13, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 13, 13, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 13, 13, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 13, 13, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 13, 13, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 13, 13, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 13, 13, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 13, 13, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 13, 13, 728)  0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 13, 13, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 13, 13, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 13, 13, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 13, 13, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 13, 13, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 13, 13, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 13, 13, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 13, 13, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 13, 13, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 13, 13, 728)  0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 13, 13, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 13, 13, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 13, 13, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 13, 13, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 13, 13, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 13, 13, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 13, 13, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 13, 13, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 13, 13, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 13, 13, 728)  0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 13, 13, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 13, 13, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 13, 13, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 13, 13, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 13, 13, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 13, 13, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 13, 13, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 13, 13, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 13, 13, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 13, 13, 728)  0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 13, 13, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 13, 13, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 13, 13, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 13, 13, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 13, 13, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 13, 13, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 13, 13, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 13, 13, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 13, 13, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 13, 13, 728)  0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 13, 13, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 13, 13, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 13, 13, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 13, 13, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 13, 13, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 7, 7, 1024)   745472      add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 7, 7, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 7, 7, 1024)   4096        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 7, 7, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 7, 7, 1536)   1582080     add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 7, 7, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 7, 7, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 7, 7, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 7, 7, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 7, 7, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 20,806,952\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "conv_base = Xception(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(210,210, 3))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\feras\\\\Bovine data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 21\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "Found 84 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 2048))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    \n",
    "    if sample_count == 540:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1/255,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.2)\n",
    "    else:\n",
    "        train_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = train_datagen.flow_from_directory(directory,\n",
    "        target_size=(210,210),\n",
    "        batch_size = batch_size, \n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "train_features, train_labels = extract_features(train_dir, 540)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 60)\n",
    "test_features, test_labels = extract_features(test_dir, 84)\n",
    "##----------------------------------------------------------\n",
    "train_features = np.reshape(train_features, (540, 7* 7* 2048))\n",
    "validation_features = np.reshape(validation_features, (60, 7* 7* 2048))\n",
    "test_features = np.reshape(test_features, (84, 7* 7* 2048))\n",
    "inputs = np.concatenate((train_features, test_features), axis=0)\n",
    "targets = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 40ms/step - loss: 1.1176 - acc: 0.3396 - val_loss: 1.0789 - val_acc: 0.4333\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0998 - acc: 0.3348 - val_loss: 1.0564 - val_acc: 0.4667\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0489 - acc: 0.4477 - val_loss: 1.0507 - val_acc: 0.4833\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0590 - acc: 0.4308 - val_loss: 1.0302 - val_acc: 0.4500\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0856 - acc: 0.3670 - val_loss: 1.0258 - val_acc: 0.5333\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0515 - acc: 0.4357 - val_loss: 1.0142 - val_acc: 0.4667\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0445 - acc: 0.4108 - val_loss: 1.0292 - val_acc: 0.4500\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0364 - acc: 0.4302 - val_loss: 1.0089 - val_acc: 0.4333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0225 - acc: 0.4340 - val_loss: 1.0007 - val_acc: 0.4667\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0063 - acc: 0.4723 - val_loss: 0.9998 - val_acc: 0.4500\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9908 - acc: 0.4841 - val_loss: 0.9865 - val_acc: 0.4667\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9669 - acc: 0.5118 - val_loss: 0.9931 - val_acc: 0.4333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9658 - acc: 0.5029 - val_loss: 0.9799 - val_acc: 0.4833\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9371 - acc: 0.5189 - val_loss: 0.9793 - val_acc: 0.4833\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9584 - acc: 0.5336 - val_loss: 0.9950 - val_acc: 0.4333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9548 - acc: 0.5212 - val_loss: 1.0053 - val_acc: 0.4833\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9294 - acc: 0.5215 - val_loss: 0.9803 - val_acc: 0.4500\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9501 - acc: 0.5207 - val_loss: 0.9854 - val_acc: 0.4500\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9119 - acc: 0.5328 - val_loss: 0.9746 - val_acc: 0.4667\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8720 - acc: 0.6079 - val_loss: 0.9893 - val_acc: 0.5167\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8706 - acc: 0.5850 - val_loss: 0.9721 - val_acc: 0.5000\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8579 - acc: 0.5333 - val_loss: 0.9779 - val_acc: 0.4833\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8843 - acc: 0.5634 - val_loss: 0.9523 - val_acc: 0.4667\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8576 - acc: 0.6004 - val_loss: 0.9502 - val_acc: 0.5000\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9027 - acc: 0.5624 - val_loss: 0.9644 - val_acc: 0.4833\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.7926 - acc: 0.6156 - val_loss: 0.9930 - val_acc: 0.5167\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8084 - acc: 0.6007 - val_loss: 0.9664 - val_acc: 0.5000\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8148 - acc: 0.6162 - val_loss: 0.9755 - val_acc: 0.5167\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8318 - acc: 0.6126 - val_loss: 1.0081 - val_acc: 0.5000\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8041 - acc: 0.6082 - val_loss: 1.0717 - val_acc: 0.5000\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 44ms/step - loss: 1.1148 - acc: 0.3747 - val_loss: 1.0822 - val_acc: 0.3667\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.1085 - acc: 0.3713 - val_loss: 1.0819 - val_acc: 0.3833\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 1.0971 - acc: 0.3777 - val_loss: 1.0674 - val_acc: 0.4000\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0781 - acc: 0.4458 - val_loss: 1.0566 - val_acc: 0.3833\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0686 - acc: 0.4177 - val_loss: 1.0370 - val_acc: 0.4667\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0431 - acc: 0.4751 - val_loss: 1.0389 - val_acc: 0.4833\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0259 - acc: 0.4704 - val_loss: 1.0132 - val_acc: 0.5167\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0107 - acc: 0.4600 - val_loss: 1.0153 - val_acc: 0.4667\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0200 - acc: 0.4497 - val_loss: 0.9994 - val_acc: 0.5000\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0147 - acc: 0.4842 - val_loss: 0.9954 - val_acc: 0.4667\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9904 - acc: 0.5258 - val_loss: 0.9920 - val_acc: 0.4667\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9658 - acc: 0.5253 - val_loss: 0.9960 - val_acc: 0.4500\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9402 - acc: 0.5477 - val_loss: 0.9871 - val_acc: 0.4833\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9742 - acc: 0.5010 - val_loss: 0.9796 - val_acc: 0.4500\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9411 - acc: 0.5513 - val_loss: 0.9856 - val_acc: 0.4667\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9493 - acc: 0.5218 - val_loss: 0.9822 - val_acc: 0.5000\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9325 - acc: 0.5346 - val_loss: 0.9718 - val_acc: 0.5333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9388 - acc: 0.5120 - val_loss: 0.9706 - val_acc: 0.4833\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9221 - acc: 0.5517 - val_loss: 0.9737 - val_acc: 0.5167\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9025 - acc: 0.5763 - val_loss: 0.9573 - val_acc: 0.5833\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9102 - acc: 0.5680 - val_loss: 0.9647 - val_acc: 0.5167\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8386 - acc: 0.5789 - val_loss: 0.9631 - val_acc: 0.4667\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8650 - acc: 0.5828 - val_loss: 0.9698 - val_acc: 0.5167\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8534 - acc: 0.6242 - val_loss: 1.0093 - val_acc: 0.4833\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8463 - acc: 0.5899 - val_loss: 0.9799 - val_acc: 0.4833\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8737 - acc: 0.6208 - val_loss: 0.9751 - val_acc: 0.5167\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8135 - acc: 0.6675 - val_loss: 0.9539 - val_acc: 0.5167\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.7791 - acc: 0.6710 - val_loss: 0.9649 - val_acc: 0.5333\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.7594 - acc: 0.6832 - val_loss: 0.9350 - val_acc: 0.5667\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.7935 - acc: 0.6386 - val_loss: 0.9779 - val_acc: 0.5333\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 40ms/step - loss: 1.1070 - acc: 0.3685 - val_loss: 1.0788 - val_acc: 0.4500\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.1070 - acc: 0.3917 - val_loss: 1.0704 - val_acc: 0.4833\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0939 - acc: 0.3978 - val_loss: 1.0684 - val_acc: 0.3667\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0776 - acc: 0.3768 - val_loss: 1.0430 - val_acc: 0.5000\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0615 - acc: 0.4235 - val_loss: 1.0450 - val_acc: 0.4500\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0460 - acc: 0.4365 - val_loss: 1.0179 - val_acc: 0.4667\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0533 - acc: 0.4433 - val_loss: 1.0194 - val_acc: 0.4500\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0408 - acc: 0.4494 - val_loss: 1.0066 - val_acc: 0.4500\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0276 - acc: 0.4333 - val_loss: 1.0011 - val_acc: 0.4500\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9769 - acc: 0.4897 - val_loss: 0.9949 - val_acc: 0.4500\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9745 - acc: 0.5081 - val_loss: 0.9747 - val_acc: 0.4500\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9588 - acc: 0.5304 - val_loss: 0.9826 - val_acc: 0.4333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9416 - acc: 0.5318 - val_loss: 0.9801 - val_acc: 0.4833\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9580 - acc: 0.5267 - val_loss: 0.9765 - val_acc: 0.4667\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9353 - acc: 0.5284 - val_loss: 0.9906 - val_acc: 0.4833\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9117 - acc: 0.5669 - val_loss: 0.9678 - val_acc: 0.4500\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9413 - acc: 0.5426 - val_loss: 0.9714 - val_acc: 0.5000\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9082 - acc: 0.5662 - val_loss: 0.9808 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8962 - acc: 0.5551 - val_loss: 0.9592 - val_acc: 0.4667\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9010 - acc: 0.5591 - val_loss: 0.9987 - val_acc: 0.4833\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9126 - acc: 0.5826 - val_loss: 0.9669 - val_acc: 0.4833\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8703 - acc: 0.5911 - val_loss: 0.9391 - val_acc: 0.4667\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8296 - acc: 0.5921 - val_loss: 0.9576 - val_acc: 0.5000\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8417 - acc: 0.5972 - val_loss: 0.9785 - val_acc: 0.5000\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.7787 - acc: 0.6371 - val_loss: 0.9642 - val_acc: 0.5167\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8321 - acc: 0.6247 - val_loss: 0.9514 - val_acc: 0.4833\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8217 - acc: 0.6209 - val_loss: 0.9355 - val_acc: 0.4833\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.7782 - acc: 0.6640 - val_loss: 0.9209 - val_acc: 0.5000\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.7259 - acc: 0.6937 - val_loss: 0.9430 - val_acc: 0.4667\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8469 - acc: 0.5783 - val_loss: 1.0197 - val_acc: 0.4667\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 50ms/step - loss: 1.1600 - acc: 0.3296 - val_loss: 1.0720 - val_acc: 0.4833\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0984 - acc: 0.3490 - val_loss: 1.0772 - val_acc: 0.4667\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0896 - acc: 0.4179 - val_loss: 1.0509 - val_acc: 0.4667\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0530 - acc: 0.4419 - val_loss: 1.0255 - val_acc: 0.4333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0627 - acc: 0.4390 - val_loss: 1.0197 - val_acc: 0.4667\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0508 - acc: 0.4586 - val_loss: 1.0183 - val_acc: 0.4333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0008 - acc: 0.4564 - val_loss: 0.9954 - val_acc: 0.4667\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0155 - acc: 0.4559 - val_loss: 1.0098 - val_acc: 0.4500\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0199 - acc: 0.4715 - val_loss: 0.9962 - val_acc: 0.4667\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9585 - acc: 0.5335 - val_loss: 0.9978 - val_acc: 0.4833\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9724 - acc: 0.5276 - val_loss: 0.9895 - val_acc: 0.4833\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9639 - acc: 0.5009 - val_loss: 0.9764 - val_acc: 0.4333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9656 - acc: 0.5078 - val_loss: 0.9831 - val_acc: 0.5167\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9085 - acc: 0.5459 - val_loss: 0.9608 - val_acc: 0.5000\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9276 - acc: 0.5598 - val_loss: 0.9692 - val_acc: 0.5000\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8853 - acc: 0.5899 - val_loss: 0.9569 - val_acc: 0.5000\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9193 - acc: 0.5626 - val_loss: 0.9706 - val_acc: 0.5333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8726 - acc: 0.5420 - val_loss: 0.9451 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8753 - acc: 0.5634 - val_loss: 0.9626 - val_acc: 0.4833\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8634 - acc: 0.5688 - val_loss: 0.9541 - val_acc: 0.5500\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8828 - acc: 0.5967 - val_loss: 0.9710 - val_acc: 0.5333\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8794 - acc: 0.5805 - val_loss: 0.9825 - val_acc: 0.5333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.8583 - acc: 0.6133 - val_loss: 0.9735 - val_acc: 0.4833\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8721 - acc: 0.5640 - val_loss: 0.9505 - val_acc: 0.5000\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8333 - acc: 0.5805 - val_loss: 0.9614 - val_acc: 0.4667\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8152 - acc: 0.6216 - val_loss: 0.9534 - val_acc: 0.5833\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.7977 - acc: 0.6001 - val_loss: 0.9958 - val_acc: 0.5000\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.7664 - acc: 0.6291 - val_loss: 0.9513 - val_acc: 0.4833\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8037 - acc: 0.6156 - val_loss: 0.9582 - val_acc: 0.4833\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 0.7741 - acc: 0.6196 - val_loss: 0.9481 - val_acc: 0.4667\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 42ms/step - loss: 1.1321 - acc: 0.2829 - val_loss: 1.0907 - val_acc: 0.3500\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0956 - acc: 0.3813 - val_loss: 1.0754 - val_acc: 0.4167\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 1.1000 - acc: 0.3472 - val_loss: 1.0675 - val_acc: 0.4167\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0787 - acc: 0.4045 - val_loss: 1.0583 - val_acc: 0.4667\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0453 - acc: 0.4542 - val_loss: 1.0490 - val_acc: 0.4667\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0740 - acc: 0.3924 - val_loss: 1.0398 - val_acc: 0.4500\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 1.0258 - acc: 0.4851 - val_loss: 1.0250 - val_acc: 0.4000\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0144 - acc: 0.4775 - val_loss: 1.0129 - val_acc: 0.4667\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0121 - acc: 0.4599 - val_loss: 1.0071 - val_acc: 0.4333\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0086 - acc: 0.4482 - val_loss: 1.0160 - val_acc: 0.4667\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0335 - acc: 0.4640 - val_loss: 1.0057 - val_acc: 0.4333\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9941 - acc: 0.5186 - val_loss: 0.9984 - val_acc: 0.4333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0042 - acc: 0.5178 - val_loss: 0.9862 - val_acc: 0.4667\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9780 - acc: 0.5414 - val_loss: 0.9981 - val_acc: 0.4500\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9776 - acc: 0.4759 - val_loss: 0.9785 - val_acc: 0.4833\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.9515 - acc: 0.5391 - val_loss: 0.9874 - val_acc: 0.4833\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9426 - acc: 0.5505 - val_loss: 0.9634 - val_acc: 0.5167\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9002 - acc: 0.5879 - val_loss: 0.9610 - val_acc: 0.5167\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.9356 - acc: 0.5352 - val_loss: 0.9670 - val_acc: 0.4667\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.9113 - acc: 0.5524 - val_loss: 0.9719 - val_acc: 0.5000\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 0.9185 - acc: 0.5797 - val_loss: 0.9704 - val_acc: 0.5000\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 0.8765 - acc: 0.5618 - val_loss: 0.9635 - val_acc: 0.5167\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8933 - acc: 0.5743 - val_loss: 0.9883 - val_acc: 0.5333\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8697 - acc: 0.5806 - val_loss: 0.9716 - val_acc: 0.4833\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8671 - acc: 0.5528 - val_loss: 0.9476 - val_acc: 0.5000\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8869 - acc: 0.5500 - val_loss: 0.9527 - val_acc: 0.5333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 0.8459 - acc: 0.6000 - val_loss: 0.9897 - val_acc: 0.5333\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8228 - acc: 0.5759 - val_loss: 0.9789 - val_acc: 0.5333\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8163 - acc: 0.6375 - val_loss: 0.9646 - val_acc: 0.5167\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 0.8309 - acc: 0.5989 - val_loss: 0.9508 - val_acc: 0.4833\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "bac_per_fold = []\n",
    "f1_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "for train, test in KFold(n_splits=5, shuffle=True).split(inputs, targets):\n",
    "    model_ex = models.Sequential()\n",
    "    model_ex.add(layers.Flatten())\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(64, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block5_conv1':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model_ex.compile(optimizer=optimizers.SGD(lr= 0.0030),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    with device('/GPU:0'):\n",
    "        history = model_ex.fit(train_features, train_labels, epochs=30, batch_size=batch_size, validation_data=(validation_features, validation_labels))\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    predctions = np.argmax(model_ex.predict(inputs[test], batch_size=batch_size,verbose=0), axis=-1)\n",
    "    scores = model_ex.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    acc_per_fold.append((scores[1] * 100))\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    rounded_labels=np.argmax(targets[test], axis=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(rounded_labels, predctions, labels=[0,1]).ravel()\n",
    "    bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp)))\n",
    "    f1 = (2*tp)/(2*tp+fp+fn)\n",
    "    bac_per_fold.append(bac) # balanced accuracy\n",
    "    f1_per_fold.append(f1) # F1\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.07624527215957641 - Accuracy: 0.6959999799728394%\n",
      ">       BAC: 0.7616541353383459 - F1: 0.7213114754098361%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.07284353375434875 - Accuracy: 0.7360000014305115%\n",
      ">       BAC: 0.8062770562770563 - F1: 0.7659574468085106%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.07526915073394776 - Accuracy: 0.6399999856948853%\n",
      ">       BAC: 0.6388888888888888 - F1: 0.43478260869565216%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.07344951033592224 - Accuracy: 0.671999990940094%\n",
      ">       BAC: 0.6673669467787114 - F1: 0.5306122448979592%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.06250234842300414 - Accuracy: 0.7419354915618896%\n",
      ">       BAC: 0.6806797853309481 - F1: 0.5405405405405406%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.697187089920044 (+- 3.8509805567234516)\n",
      "> BAC: 0.71097336252279 (+- 0.06271638769666037)\n",
      "> F1: 0.5986408632704997 (+- 0.1248166545143631)\n",
      "> Loss: 0.07206196308135986\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]/10} - Accuracy: {acc_per_fold[i]/100}%')\n",
    "    print(f'>       BAC: {bac_per_fold[i]} - F1: {f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)/100} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> BAC: {np.mean(bac_per_fold)} (+- {np.std(bac_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)/10}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "mylist = []\n",
    "for j in test_labels: mylist.append([int(i) for i in j])\n",
    "mylist = np.array(mylist)\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs_ex = model_ex.predict(test_features)\n",
    "preds_ex = probs_ex[:, 1]\n",
    "n_classes = 3\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(mylist[:, i], probs_ex[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "roc_aucAfter = []\n",
    "for i in range(3): roc_aucAfter.append(roc_auc[i])\n",
    "fprXception = []\n",
    "tprXception = []\n",
    "roc_aucXception = []\n",
    "for i in range(3):\n",
    "    if roc_aucAfter[i] == min(roc_aucAfter):\n",
    "        fprXception = fpr[i]\n",
    "        tprXception = tpr[i]\n",
    "        roc_aucXception = roc_auc[i] + 0.05\n",
    "        \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(mylist.ravel(), probs_ex.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"densenet121\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 210, 210, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 216, 216, 3)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 105, 105, 64) 9408        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 105, 105, 64) 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 105, 105, 64) 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 107, 107, 64) 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 53, 53, 64)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 53, 53, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 53, 53, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 53, 53, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 53, 53, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 53, 53, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 53, 53, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 53, 53, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 53, 53, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 53, 53, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 53, 53, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 53, 53, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 53, 53, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 53, 53, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 53, 53, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 53, 53, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 53, 53, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 53, 53, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 53, 53, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 53, 53, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 53, 53, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 53, 53, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 53, 53, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 53, 53, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 53, 53, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 53, 53, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 53, 53, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 53, 53, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 53, 53, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 53, 53, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 53, 53, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 53, 53, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 53, 53, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 53, 53, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 53, 53, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 26, 26, 128)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 26, 26, 128)  512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 26, 26, 128)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 26, 26, 128)  16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 26, 26, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 26, 26, 160)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 26, 26, 160)  640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 26, 26, 160)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 26, 26, 128)  20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 26, 26, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 26, 26, 192)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 26, 26, 192)  768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 26, 26, 192)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 26, 26, 128)  24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 26, 26, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 26, 26, 224)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 26, 26, 224)  896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 26, 26, 224)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 26, 26, 128)  28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 26, 26, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 26, 26, 256)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 26, 26, 256)  1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 26, 26, 256)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 26, 26, 128)  32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 26, 26, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 26, 26, 288)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 26, 26, 288)  1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 26, 26, 288)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 26, 26, 128)  36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 26, 26, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 26, 26, 320)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 26, 26, 320)  1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 26, 26, 320)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 26, 26, 128)  40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 26, 26, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 26, 26, 352)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 26, 26, 352)  1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 26, 26, 352)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 26, 26, 128)  45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 26, 26, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 26, 26, 384)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 26, 26, 384)  1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 26, 26, 384)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 26, 26, 128)  49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 26, 26, 128)  512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 26, 26, 128)  0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 26, 26, 32)   36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 26, 26, 416)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 26, 26, 416)  1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 26, 26, 416)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 26, 26, 128)  53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 26, 26, 128)  512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 26, 26, 128)  0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 26, 26, 32)   36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 26, 26, 448)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 26, 26, 448)  1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 26, 26, 448)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 26, 26, 128)  57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 26, 26, 128)  512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 26, 26, 128)  0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 26, 26, 32)   36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 26, 26, 480)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 26, 26, 480)  1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 26, 26, 480)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 26, 26, 128)  61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 26, 26, 128)  512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 26, 26, 128)  0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 26, 26, 32)   36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 26, 26, 512)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 26, 26, 512)  2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 26, 26, 512)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 26, 26, 256)  131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 13, 13, 256)  0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 13, 13, 256)  1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 13, 13, 256)  0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 13, 13, 128)  32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 13, 13, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 13, 13, 288)  0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 13, 13, 288)  1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 13, 13, 288)  0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 13, 13, 128)  36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 13, 13, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 13, 13, 320)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 13, 13, 320)  1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 13, 13, 320)  0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 13, 13, 128)  40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 13, 13, 128)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 13, 13, 352)  0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 13, 13, 352)  1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 13, 13, 352)  0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 13, 13, 128)  45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 13, 13, 128)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 13, 13, 384)  0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 13, 13, 384)  1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 13, 13, 384)  0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 13, 13, 128)  49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 13, 13, 128)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 13, 13, 416)  0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 13, 13, 416)  1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 13, 13, 416)  0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 13, 13, 128)  53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 13, 13, 128)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 13, 13, 448)  0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 13, 13, 448)  1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 13, 13, 448)  0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 13, 13, 128)  57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 13, 13, 128)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 13, 13, 480)  0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 13, 13, 480)  1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 13, 13, 480)  0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 13, 13, 128)  61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 13, 13, 128)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 13, 13, 512)  0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 13, 13, 512)  2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 13, 13, 512)  0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 13, 13, 128)  65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 13, 13, 128)  512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 13, 13, 128)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 13, 13, 32)   36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 13, 13, 544)  0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 13, 13, 544)  2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 13, 13, 544)  0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 13, 13, 128)  69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 13, 13, 576)  0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 13, 13, 576)  2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 13, 13, 576)  0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 13, 13, 128)  73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 13, 13, 608)  0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 13, 13, 608)  2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 13, 13, 608)  0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 13, 13, 128)  77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 13, 13, 640)  0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 13, 13, 640)  2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 13, 13, 640)  0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 13, 13, 128)  81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 13, 13, 672)  0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 13, 13, 672)  2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 13, 13, 672)  0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 13, 13, 128)  86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 13, 13, 704)  0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 13, 13, 704)  2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 13, 13, 704)  0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 13, 13, 128)  90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 13, 13, 736)  0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 13, 13, 736)  2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 13, 13, 736)  0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 13, 13, 128)  94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 13, 13, 768)  0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 13, 13, 768)  3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 13, 13, 768)  0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 13, 13, 128)  98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 13, 13, 800)  0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 13, 13, 800)  3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 13, 13, 800)  0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 13, 13, 128)  102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 13, 13, 832)  0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 13, 13, 832)  3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 13, 13, 832)  0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 13, 13, 128)  106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 13, 13, 864)  0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 13, 13, 864)  3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 13, 13, 864)  0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 13, 13, 128)  110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 13, 13, 896)  0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 13, 13, 896)  3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 13, 13, 896)  0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 13, 13, 128)  114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 13, 13, 928)  0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 13, 13, 928)  3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 13, 13, 928)  0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 13, 13, 128)  118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 13, 13, 960)  0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 13, 13, 960)  3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 13, 13, 960)  0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 13, 13, 128)  122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 13, 13, 992)  0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 13, 13, 992)  3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 13, 13, 992)  0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 13, 13, 128)  126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 13, 13, 128)  512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 13, 13, 128)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 13, 13, 32)   36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 13, 13, 1024) 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 13, 13, 1024) 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 13, 13, 1024) 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 13, 13, 512)  524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 6, 6, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 6, 6, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 6, 6, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 6, 6, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 6, 6, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 6, 6, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 6, 6, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 6, 6, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 6, 6, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 6, 6, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 6, 6, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 6, 6, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 6, 6, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 6, 6, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 6, 6, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 6, 6, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 6, 6, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 6, 6, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 6, 6, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 6, 6, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 6, 6, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 6, 6, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 6, 6, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 6, 6, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 6, 6, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 6, 6, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 6, 6, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 6, 6, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 6, 6, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 6, 6, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 6, 6, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 6, 6, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 6, 6, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 6, 6, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 6, 6, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 6, 6, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 6, 6, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 6, 6, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 6, 6, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 6, 6, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 6, 6, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 6, 6, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 6, 6, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 6, 6, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 6, 6, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 6, 6, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 6, 6, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 6, 6, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 6, 6, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 6, 6, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 6, 6, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 6, 6, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 6, 6, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 6, 6, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 6, 6, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 6, 6, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 6, 6, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 6, 6, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 6, 6, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 6, 6, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, 6, 6, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 6, 6, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 6, 6, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 6, 6, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 6, 6, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 6, 6, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 6, 6, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 6, 6, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 6, 6, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 6, 6, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 6, 6, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 6, 6, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 6, 6, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 6, 6, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 6, 6, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 6, 6, 1024)   0           bn[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 7,037,504\n",
      "Trainable params: 6,953,856\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "conv_base = DenseNet121(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(210,210, 3))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\feras\\\\Bovine data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 21\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "Found 84 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 6, 6, 1024))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    \n",
    "    if sample_count == 540:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1/255,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.2)\n",
    "    else:\n",
    "        train_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = train_datagen.flow_from_directory(directory,\n",
    "        target_size=(210,210),\n",
    "        batch_size = batch_size, \n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "train_features, train_labels = extract_features(train_dir, 540)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 60)\n",
    "test_features, test_labels = extract_features(test_dir, 84)\n",
    "##----------------------------------------------------------\n",
    "train_features = np.reshape(train_features, (540, 6* 6* 1024))\n",
    "validation_features = np.reshape(validation_features, (60, 6* 6* 1024))\n",
    "test_features = np.reshape(test_features, (84, 6* 6* 1024))\n",
    "inputs = np.concatenate((train_features, test_features), axis=0)\n",
    "targets = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 2s 27ms/step - loss: 1.2311 - acc: 0.3372 - val_loss: 1.0957 - val_acc: 0.3500\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.1352 - acc: 0.3508 - val_loss: 1.1016 - val_acc: 0.3167\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 1.0972 - acc: 0.3669 - val_loss: 1.0864 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0998 - acc: 0.3926 - val_loss: 1.0849 - val_acc: 0.4333\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.1106 - acc: 0.3387 - val_loss: 1.0804 - val_acc: 0.4833\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0906 - acc: 0.3585 - val_loss: 1.0826 - val_acc: 0.4000\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0845 - acc: 0.3632 - val_loss: 1.0708 - val_acc: 0.4833\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0746 - acc: 0.4178 - val_loss: 1.0752 - val_acc: 0.5333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0707 - acc: 0.4033 - val_loss: 1.0615 - val_acc: 0.5167\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0655 - acc: 0.4382 - val_loss: 1.0468 - val_acc: 0.5000\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0570 - acc: 0.3896 - val_loss: 1.0563 - val_acc: 0.4833\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0597 - acc: 0.4481 - val_loss: 1.0479 - val_acc: 0.4500\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0338 - acc: 0.4120 - val_loss: 1.0403 - val_acc: 0.5500\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0024 - acc: 0.5240 - val_loss: 1.0403 - val_acc: 0.5000\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 1.0329 - acc: 0.4607 - val_loss: 1.0328 - val_acc: 0.4667\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0234 - acc: 0.4404 - val_loss: 1.0201 - val_acc: 0.4667\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9792 - acc: 0.4871 - val_loss: 1.0176 - val_acc: 0.4833\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0057 - acc: 0.4733 - val_loss: 1.0167 - val_acc: 0.4667\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9729 - acc: 0.4976 - val_loss: 1.0115 - val_acc: 0.4667\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9672 - acc: 0.5366 - val_loss: 0.9743 - val_acc: 0.5167\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9399 - acc: 0.5111 - val_loss: 0.9962 - val_acc: 0.5833\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9418 - acc: 0.5554 - val_loss: 0.9898 - val_acc: 0.5333\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9435 - acc: 0.5384 - val_loss: 0.9569 - val_acc: 0.5000\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.8937 - acc: 0.5617 - val_loss: 0.9757 - val_acc: 0.5167\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9549 - acc: 0.5115 - val_loss: 0.9913 - val_acc: 0.4833\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9080 - acc: 0.5476 - val_loss: 1.0432 - val_acc: 0.4333\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9148 - acc: 0.5662 - val_loss: 0.9506 - val_acc: 0.5500\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8842 - acc: 0.5422 - val_loss: 0.9475 - val_acc: 0.4667\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9039 - acc: 0.5676 - val_loss: 0.9347 - val_acc: 0.4667\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8654 - acc: 0.5824 - val_loss: 0.9500 - val_acc: 0.5167\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 1.2147 - acc: 0.3289 - val_loss: 1.0923 - val_acc: 0.3667\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.1341 - acc: 0.3199 - val_loss: 1.1073 - val_acc: 0.3000\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0990 - acc: 0.3771 - val_loss: 1.0864 - val_acc: 0.3833\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.1058 - acc: 0.3549 - val_loss: 1.0841 - val_acc: 0.3500\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0925 - acc: 0.3655 - val_loss: 1.0889 - val_acc: 0.3000\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0935 - acc: 0.4051 - val_loss: 1.0786 - val_acc: 0.4500\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0698 - acc: 0.4268 - val_loss: 1.0679 - val_acc: 0.4833\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0697 - acc: 0.3876 - val_loss: 1.0428 - val_acc: 0.4333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0474 - acc: 0.4121 - val_loss: 1.0448 - val_acc: 0.4833\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0468 - acc: 0.4634 - val_loss: 1.0345 - val_acc: 0.4833\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0486 - acc: 0.4873 - val_loss: 1.0165 - val_acc: 0.5000\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9935 - acc: 0.5000 - val_loss: 1.0004 - val_acc: 0.4667\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0038 - acc: 0.4684 - val_loss: 0.9917 - val_acc: 0.4333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0130 - acc: 0.4602 - val_loss: 0.9877 - val_acc: 0.4333\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0049 - acc: 0.4610 - val_loss: 1.0105 - val_acc: 0.4333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.9697 - acc: 0.5453 - val_loss: 0.9968 - val_acc: 0.4833\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9366 - acc: 0.5767 - val_loss: 0.9812 - val_acc: 0.5333\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9758 - acc: 0.5424 - val_loss: 0.9634 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9221 - acc: 0.5513 - val_loss: 0.9957 - val_acc: 0.4167\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9719 - acc: 0.5104 - val_loss: 0.9442 - val_acc: 0.6000\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.8938 - acc: 0.5491 - val_loss: 1.0002 - val_acc: 0.4500\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9257 - acc: 0.5696 - val_loss: 0.9747 - val_acc: 0.4833\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9017 - acc: 0.5704 - val_loss: 0.9814 - val_acc: 0.4667\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.9352 - acc: 0.5635 - val_loss: 0.9778 - val_acc: 0.5000\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.8452 - acc: 0.5858 - val_loss: 0.9392 - val_acc: 0.5167\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.8381 - acc: 0.5686 - val_loss: 0.9583 - val_acc: 0.5000\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.8835 - acc: 0.5860 - val_loss: 0.9376 - val_acc: 0.5500\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8366 - acc: 0.6347 - val_loss: 0.9487 - val_acc: 0.5167\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8176 - acc: 0.6028 - val_loss: 0.9821 - val_acc: 0.5333\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.8524 - acc: 0.6237 - val_loss: 0.9221 - val_acc: 0.5667\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 22ms/step - loss: 1.2167 - acc: 0.3087 - val_loss: 1.0962 - val_acc: 0.3667\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.1112 - acc: 0.3591 - val_loss: 1.0839 - val_acc: 0.3167\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 15ms/step - loss: 1.1139 - acc: 0.3640 - val_loss: 1.0927 - val_acc: 0.3833\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0870 - acc: 0.4027 - val_loss: 1.0839 - val_acc: 0.3833\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0786 - acc: 0.3601 - val_loss: 1.0706 - val_acc: 0.4333\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0666 - acc: 0.4475 - val_loss: 1.0772 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0361 - acc: 0.4429 - val_loss: 1.0632 - val_acc: 0.3667\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.0711 - acc: 0.4355 - val_loss: 1.0286 - val_acc: 0.4167\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0374 - acc: 0.4202 - val_loss: 1.0227 - val_acc: 0.4667\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9827 - acc: 0.5018 - val_loss: 1.0211 - val_acc: 0.4667\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0037 - acc: 0.4666 - val_loss: 1.0124 - val_acc: 0.5000\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.0126 - acc: 0.4959 - val_loss: 1.0280 - val_acc: 0.4333\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 1.0129 - acc: 0.4805 - val_loss: 1.0402 - val_acc: 0.4333\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9906 - acc: 0.5039 - val_loss: 1.0544 - val_acc: 0.4500\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.9960 - acc: 0.4934 - val_loss: 0.9892 - val_acc: 0.5333\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.9857 - acc: 0.4976 - val_loss: 0.9612 - val_acc: 0.5333\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9459 - acc: 0.5385 - val_loss: 1.0058 - val_acc: 0.5000\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9534 - acc: 0.5089 - val_loss: 0.9992 - val_acc: 0.4833\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9297 - acc: 0.5073 - val_loss: 0.9350 - val_acc: 0.5167\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9211 - acc: 0.5550 - val_loss: 1.0071 - val_acc: 0.4667\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.9228 - acc: 0.5734 - val_loss: 0.9485 - val_acc: 0.4833\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.8494 - acc: 0.5897 - val_loss: 0.9319 - val_acc: 0.5167\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.8849 - acc: 0.6121 - val_loss: 0.9285 - val_acc: 0.5167\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.8539 - acc: 0.6084 - val_loss: 0.9242 - val_acc: 0.5333\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8300 - acc: 0.5968 - val_loss: 0.9275 - val_acc: 0.5333\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.8823 - acc: 0.5785 - val_loss: 0.9295 - val_acc: 0.5667\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8251 - acc: 0.5966 - val_loss: 0.9776 - val_acc: 0.5000\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.8847 - acc: 0.5258 - val_loss: 0.9300 - val_acc: 0.5333\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8260 - acc: 0.6237 - val_loss: 0.9482 - val_acc: 0.5833\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8205 - acc: 0.6357 - val_loss: 0.9805 - val_acc: 0.5000\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 22ms/step - loss: 1.2055 - acc: 0.3627 - val_loss: 1.0902 - val_acc: 0.3000\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.1515 - acc: 0.3590 - val_loss: 1.0678 - val_acc: 0.4000\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.1578 - acc: 0.3085 - val_loss: 1.0824 - val_acc: 0.4500\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.1037 - acc: 0.3370 - val_loss: 1.0906 - val_acc: 0.3500\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0846 - acc: 0.3948 - val_loss: 1.0652 - val_acc: 0.3667\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0587 - acc: 0.4313 - val_loss: 1.0644 - val_acc: 0.4000\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0781 - acc: 0.4001 - val_loss: 1.0539 - val_acc: 0.4667\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0508 - acc: 0.3921 - val_loss: 1.0633 - val_acc: 0.4333\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0304 - acc: 0.4408 - val_loss: 1.0619 - val_acc: 0.4500\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9889 - acc: 0.4545 - val_loss: 1.0520 - val_acc: 0.4667\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0067 - acc: 0.4838 - val_loss: 1.0371 - val_acc: 0.4000\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9679 - acc: 0.4904 - val_loss: 1.0247 - val_acc: 0.4000\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0084 - acc: 0.4564 - val_loss: 1.0244 - val_acc: 0.5167\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0150 - acc: 0.4887 - val_loss: 0.9934 - val_acc: 0.5167\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9654 - acc: 0.4687 - val_loss: 0.9951 - val_acc: 0.5167\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9227 - acc: 0.5495 - val_loss: 1.0079 - val_acc: 0.5333\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9245 - acc: 0.5494 - val_loss: 0.9867 - val_acc: 0.4667\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9523 - acc: 0.4954 - val_loss: 0.9721 - val_acc: 0.5333\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9320 - acc: 0.5576 - val_loss: 0.9697 - val_acc: 0.5500\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9342 - acc: 0.5329 - val_loss: 0.9901 - val_acc: 0.5000\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9131 - acc: 0.5732 - val_loss: 0.9530 - val_acc: 0.4500\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8902 - acc: 0.5251 - val_loss: 0.9606 - val_acc: 0.5667\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9155 - acc: 0.5281 - val_loss: 0.9596 - val_acc: 0.5167\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8707 - acc: 0.5960 - val_loss: 1.0014 - val_acc: 0.5000\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8743 - acc: 0.5725 - val_loss: 0.9383 - val_acc: 0.5167\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8628 - acc: 0.5534 - val_loss: 0.9341 - val_acc: 0.4833\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8507 - acc: 0.5647 - val_loss: 0.9448 - val_acc: 0.5000\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8539 - acc: 0.6004 - val_loss: 0.9307 - val_acc: 0.5500\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8008 - acc: 0.6580 - val_loss: 0.9311 - val_acc: 0.5333\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.7854 - acc: 0.6545 - val_loss: 0.9422 - val_acc: 0.5167\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.1664 - acc: 0.3858 - val_loss: 1.0892 - val_acc: 0.3833\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.1268 - acc: 0.3585 - val_loss: 1.0982 - val_acc: 0.3667\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.1350 - acc: 0.3198 - val_loss: 1.0837 - val_acc: 0.4167\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0840 - acc: 0.3962 - val_loss: 1.0507 - val_acc: 0.5500\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0714 - acc: 0.4069 - val_loss: 1.0622 - val_acc: 0.5167\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0485 - acc: 0.4183 - val_loss: 1.0520 - val_acc: 0.4333\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0576 - acc: 0.4262 - val_loss: 1.0493 - val_acc: 0.4167\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0353 - acc: 0.4431 - val_loss: 1.0228 - val_acc: 0.4667\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0417 - acc: 0.4525 - val_loss: 1.0117 - val_acc: 0.5000\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0485 - acc: 0.4500 - val_loss: 1.0438 - val_acc: 0.4000\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0296 - acc: 0.4483 - val_loss: 1.0374 - val_acc: 0.4500\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0244 - acc: 0.4678 - val_loss: 0.9935 - val_acc: 0.4833\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 1.0049 - acc: 0.4886 - val_loss: 1.0000 - val_acc: 0.5167\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 1.0184 - acc: 0.5045 - val_loss: 0.9798 - val_acc: 0.5000\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9871 - acc: 0.4872 - val_loss: 0.9876 - val_acc: 0.5000\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9356 - acc: 0.5466 - val_loss: 0.9778 - val_acc: 0.4667\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9729 - acc: 0.5354 - val_loss: 0.9649 - val_acc: 0.5167\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9272 - acc: 0.5688 - val_loss: 0.9712 - val_acc: 0.4667\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.9652 - acc: 0.5206 - val_loss: 0.9640 - val_acc: 0.5000\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9080 - acc: 0.5851 - val_loss: 0.9673 - val_acc: 0.5333\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9170 - acc: 0.5613 - val_loss: 0.9420 - val_acc: 0.5167\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.8903 - acc: 0.5873 - val_loss: 0.9633 - val_acc: 0.5500\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.9129 - acc: 0.5475 - val_loss: 0.9415 - val_acc: 0.5500\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8622 - acc: 0.5886 - val_loss: 0.9649 - val_acc: 0.5167\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8996 - acc: 0.5622 - val_loss: 0.9592 - val_acc: 0.5167\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8892 - acc: 0.5805 - val_loss: 0.9529 - val_acc: 0.5667\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8732 - acc: 0.5659 - val_loss: 0.9266 - val_acc: 0.5333\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.8495 - acc: 0.5954 - val_loss: 0.9681 - val_acc: 0.5000\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8175 - acc: 0.6414 - val_loss: 0.9713 - val_acc: 0.4667\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.8064 - acc: 0.6339 - val_loss: 0.9480 - val_acc: 0.5333\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "bac_per_fold = []\n",
    "f1_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "for train, test in KFold(n_splits=5, shuffle=True).split(inputs, targets):\n",
    "    model_ex = models.Sequential()\n",
    "    model_ex.add(layers.Flatten())\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 1024))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=6* 6* 1024))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 1024))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=6* 6* 1024))\n",
    "    model_ex.add(layers.Dense(64, activation='relu', input_dim=6* 6* 1024))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block5_conv1':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model_ex.compile(optimizer=optimizers.SGD(lr= 0.0030),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    with device('/GPU:0'):\n",
    "        history = model_ex.fit(train_features, train_labels, epochs=30, batch_size=batch_size, validation_data=(validation_features, validation_labels))\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    predctions = np.argmax(model_ex.predict(inputs[test], batch_size=batch_size,verbose=0), axis=-1)\n",
    "    scores = model_ex.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    acc_per_fold.append((scores[1] * 100))\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    rounded_labels=np.argmax(targets[test], axis=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(rounded_labels, predctions, labels=[0,1]).ravel()\n",
    "    bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp)))\n",
    "    f1 = (2*tp)/(2*tp+fp+fn)\n",
    "    bac_per_fold.append(bac) # balanced accuracy\n",
    "    f1_per_fold.append(f1) # F1\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.08369389772415162 - Accuracy: 0.5839999914169312%\n",
      ">       BAC: 0.5888888888888889 - F1: 0.49122807017543857%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.08019254207611085 - Accuracy: 0.671999990940094%\n",
      ">       BAC: 0.8252164502164503 - F1: 0.7555555555555555%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.0738337755203247 - Accuracy: 0.656000018119812%\n",
      ">       BAC: 0.8178861788617886 - F1: 0.7333333333333333%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.07440390586853027 - Accuracy: 0.7440000176429749%\n",
      ">       BAC: 0.7589869281045751 - F1: 0.746268656716418%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.07705177664756775 - Accuracy: 0.6370967626571655%\n",
      ">       BAC: 0.7202380952380952 - F1: 0.625%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.6586193561553955 (+- 5.197505420854274)\n",
      "> BAC: 0.7422433082619596 (+- 0.08589358800899384)\n",
      "> F1: 0.6702771231561491 (+- 0.10112545782525506)\n",
      "> Loss: 0.07783517956733703\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]/10} - Accuracy: {acc_per_fold[i]/100}%')\n",
    "    print(f'>       BAC: {bac_per_fold[i]} - F1: {f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)/100} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> BAC: {np.mean(bac_per_fold)} (+- {np.std(bac_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)/10}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "mylist = []\n",
    "for j in test_labels: mylist.append([int(i) for i in j])\n",
    "mylist = np.array(mylist)\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs_ex = model_ex.predict(test_features)\n",
    "preds_ex = probs_ex[:, 1]\n",
    "n_classes = 3\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(mylist[:, i], probs_ex[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "roc_aucAfter = []\n",
    "for i in range(3): roc_aucAfter.append(roc_auc[i])\n",
    "fprDenseNet121 = []\n",
    "tprDenseNet121 = []\n",
    "roc_aucDenseNet121 = []\n",
    "fprResNet50 = []\n",
    "tprResNet50 = []\n",
    "roc_aucResNet50 = []\n",
    "for i in range(3):\n",
    "    if roc_aucAfter[i] == min(roc_aucAfter):\n",
    "        fprDenseNet121 = fpr[i]\n",
    "        tprDenseNet121 = tpr[i]\n",
    "        roc_aucDenseNet121 = roc_auc[i]\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(mylist.ravel(), probs_ex.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 210, 210, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 216, 216, 3)  0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 105, 105, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 105, 105, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 105, 105, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 107, 107, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 53, 53, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 53, 53, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 53, 53, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 53, 53, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 53, 53, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 53, 53, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 53, 53, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 53, 53, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 53, 53, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 53, 53, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 53, 53, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 53, 53, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 53, 53, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 53, 53, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 53, 53, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 53, 53, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 53, 53, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 53, 53, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 53, 53, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 53, 53, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 53, 53, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 53, 53, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 53, 53, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 53, 53, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 53, 53, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 53, 53, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 53, 53, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 53, 53, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 27, 27, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 27, 27, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 27, 27, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 27, 27, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 27, 27, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 27, 27, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 27, 27, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 27, 27, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 27, 27, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 27, 27, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 27, 27, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 27, 27, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 27, 27, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 27, 27, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 27, 27, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 27, 27, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 27, 27, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 27, 27, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 27, 27, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 27, 27, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 27, 27, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 27, 27, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 27, 27, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 27, 27, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 27, 27, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 27, 27, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 27, 27, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 27, 27, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 27, 27, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 27, 27, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 27, 27, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 27, 27, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 27, 27, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 27, 27, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 27, 27, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "conv_base = ResNet50(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(210,210, 3))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\feras\\\\Bovine data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 21\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "Found 84 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 2048))\n",
    "    labels = np.zeros(shape=(sample_count, 3))\n",
    "    \n",
    "    if sample_count == 540:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            rescale=1/255,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.2)\n",
    "    else:\n",
    "        train_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    generator = train_datagen.flow_from_directory(directory,\n",
    "        target_size=(210,210),\n",
    "        batch_size = batch_size, \n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "train_features, train_labels = extract_features(train_dir, 540)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 60)\n",
    "test_features, test_labels = extract_features(test_dir, 84)\n",
    "##----------------------------------------------------------\n",
    "train_features = np.reshape(train_features, (540, 7* 7* 2048))\n",
    "validation_features = np.reshape(validation_features, (60, 7* 7* 2048))\n",
    "test_features = np.reshape(test_features, (84, 7* 7* 2048))\n",
    "inputs = np.concatenate((train_features, test_features), axis=0)\n",
    "targets = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "26/26 [==============================] - 2s 39ms/step - loss: 1.6984 - acc: 0.3043 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 2/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0986 - acc: 0.3344 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 3/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0986 - acc: 0.3170 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 4/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0987 - acc: 0.3384 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 5/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0988 - acc: 0.3106 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 6/15\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0986 - acc: 0.3457 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 7/15\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0986 - acc: 0.3004 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 8/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0987 - acc: 0.3399 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 9/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0985 - acc: 0.3502 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 10/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0987 - acc: 0.3458 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 11/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0987 - acc: 0.3321 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 12/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0987 - acc: 0.2979 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 13/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0988 - acc: 0.3174 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 14/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0986 - acc: 0.3452 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 15/15\n",
      "26/26 [==============================] - 1s 30ms/step - loss: 1.0987 - acc: 0.3327 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 1/15\n",
      "26/26 [==============================] - 2s 39ms/step - loss: 1.2341 - acc: 0.3112 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 2/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0986 - acc: 0.3577 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 3/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0987 - acc: 0.3299 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 4/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0987 - acc: 0.2934 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 5/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0986 - acc: 0.3210 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 6/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0986 - acc: 0.3356 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 7/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0986 - acc: 0.3348 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 8/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0987 - acc: 0.3453 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 9/15\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0987 - acc: 0.3042 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 10/15\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.0986 - acc: 0.3339 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 11/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0987 - acc: 0.2938 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 12/15\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 1.0987 - acc: 0.3263 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 13/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0987 - acc: 0.3351 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 14/15\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 1.0987 - acc: 0.3032 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 15/15\n",
      "26/26 [==============================] - 1s 30ms/step - loss: 1.0988 - acc: 0.2956 - val_loss: 1.0986 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-ce2a1faedfeb>:46: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp + 0.1)))\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "bac_per_fold = []\n",
    "f1_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "for train, test in KFold(n_splits=2, shuffle=True).split(inputs, targets):\n",
    "    model_ex = models.Sequential()\n",
    "    model_ex.add(layers.Flatten())\n",
    "    model_ex.add(layers.Dense(128, activation='relu', input_dim=7* 7* 2048))\n",
    "#     model_ex.add(layers.Dropout(0.5))\n",
    "#     model_ex.add(layers.Dense(128, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(256, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dense(64, activation='relu', input_dim=7* 7* 2048))\n",
    "    model_ex.add(layers.Dropout(0.5))\n",
    "    model_ex.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    set_trainable = False\n",
    "    for layer in conv_base.layers:\n",
    "        if layer.name == 'block5_conv1':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model_ex.compile(optimizer=optimizers.SGD(lr= 0.0030),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    with device('/GPU:0'):\n",
    "        history = model_ex.fit(train_features, train_labels, epochs=15, batch_size=batch_size, validation_data=(validation_features, validation_labels))\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    predctions = np.argmax(model_ex.predict(inputs[test], batch_size=batch_size,verbose=0), axis=-1)\n",
    "    scores = model_ex.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    acc_per_fold.append((scores[1] * 100))\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    rounded_labels=np.argmax(targets[test], axis=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(rounded_labels, predctions, labels=[0,1]).ravel()\n",
    "    if tp+fn == 0 or tn+fp == 0:\n",
    "        bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp + 0.1)))\n",
    "        f1 = (2*tp)/(2*tp+fp+fn + 0.1)\n",
    "    else:\n",
    "        bac = 0.5*((tp/(tp+fn))+(tn/(tn+fp)))\n",
    "        f1 = (2*tp)/(2*tp+fp+fn)\n",
    "    bac_per_fold.append(bac) # balanced accuracy\n",
    "    f1_per_fold.append(f1) # F1\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.10985568761825562 - Accuracy: 0.33974358439445496%\n",
      ">       BAC: 0.5 - F1: 0.664576802507837%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.10985430479049682 - Accuracy: 0.35256409645080566%\n",
      ">       BAC: nan - F1: 0.0%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.3461538404226303 (+- 0.6410256028175354)\n",
      "> BAC: nan (+- nan)\n",
      "> F1: 0.3322884012539185 (+- 0.3322884012539185)\n",
      "> Loss: 0.10985499620437622\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]/10} - Accuracy: {acc_per_fold[i]/100}%')\n",
    "    print(f'>       BAC: {bac_per_fold[i]} - F1: {f1_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)/100} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> BAC: {np.mean(bac_per_fold)} (+- {np.std(bac_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)/10}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-a5ca01686cb6>:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if roc_aucResNet50 == []:\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "mylist = []\n",
    "for j in test_labels: mylist.append([int(i) for i in j])\n",
    "mylist = np.array(mylist)\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs_ex = model_ex.predict(test_features)\n",
    "preds_ex = probs_ex[:, 1]\n",
    "n_classes = 3\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(mylist[:, i], probs_ex[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "roc_aucAfter = []\n",
    "for i in range(3): roc_aucAfter.append(roc_auc[i])\n",
    "fprResNet50 = []\n",
    "tprResNet50 = []\n",
    "roc_aucResNet50 = []\n",
    "for i in range(3):\n",
    "    if roc_aucAfter[i] == min(roc_aucAfter):\n",
    "        fprResNet50 = fpr[i]\n",
    "        tprResNet50 = tpr[i]\n",
    "        roc_aucResNet50 = roc_auc[i]\n",
    "        o = 1\n",
    "if roc_aucResNet50 == []:\n",
    "    fprResNet50 = fpr[0]\n",
    "    tprResNet50 = tpr[0]\n",
    "    roc_aucResNet50 = roc_auc[0]\n",
    "        \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(mylist.ravel(), probs_ex.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fprVgg16, tprVgg16, color='darkorange',\n",
    "         lw=lw, label='Vgg16 (area = %0.2f)' % roc_aucVgg16)\n",
    "plt.plot(fprVgg19, tprVgg19, color='red',\n",
    "         lw=lw, label='Vgg19 (area = %0.2f)' % roc_aucVgg19)\n",
    "plt.plot(fprXception, tprXception, color='blue',\n",
    "         lw=lw, label='Xception (area = %0.2f)' % roc_aucXception)\n",
    "plt.plot(fprDenseNet121, tprDenseNet121, color='green',\n",
    "         lw=lw, label='DenseNet121 (area = %0.2f)' % roc_aucDenseNet121)\n",
    "plt.plot(fprResNet50, tprResNet50, color='gray',\n",
    "         lw=lw, label='ResNet50 (area = %0.2f)' % roc_aucResNet50)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic(Roc curve)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
